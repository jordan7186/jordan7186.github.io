---
---

@inproceedings{Shin2023PNGLoG,
    author = {Yong-Min Shin and Won-Yong Shin},
    title = {Propagate \& Distill: Towards Effective Graph Learners Using Propagation-Embracing MLPs},
    abbr = {LoG},
    booktitle = {The Second Learning on Graphs Conference},
    address = {Virtural Event},
    year = {2023},
    month = {Nov.},
    html = {https://arxiv.org/abs/2311.17781},
    pdf = {PND_final_arxiv.pdf},
    selected = {true},
}

@article{shin2022pagetpami,
  author       = {Yong{-}Min Shin and
                  Sun{-}Woo Kim and
                  Won{-}Yong Shin},
  title        = {{PAGE:} Prototype-Based Model-Level Explanations for Graph Neural
                  Networks (under review for TPAMI)},
  journal      = {arXiv},
  volume       = {abs/2210.17159},
  year         = {2022},
  url          = {https://doi.org/10.48550/arXiv.2210.17159},
  doi          = {10.48550/ARXIV.2210.17159},
  abbr = {arXiv},
  html = {https://arxiv.org/abs/2210.17159},
  pdf = {pagetpami.pdf},
  selected = {true},
}

@inproceedings{Shin2022pageaaai,
  author       = {Yong{-}Min Shin and
                  Sun{-}Woo Kim and
                  Eun{-}Bi Yoon and
                  Won{-}Yong Shin},
  title        = {Prototype-Based Explanations for Graph Neural Networks (Student Abstract) (selected for oral presentation)},
  booktitle    = {{AAAI} Conference on Artificial Intelligence, ({AAAI})},
  pages        = {13047--13048},
  publisher    = {{AAAI} Press},
  year         = {2022},
  url          = {https://doi.org/10.1609/aaai.v36i11.21660},
  doi          = {10.1609/AAAI.V36I11.21660},
  month = {Feb.-Mar.},
  address = {Virtual Event},
  abbr = {AAAI},
  html = {https://ojs.aaai.org/index.php/AAAI/article/view/21660/21409},
  pdf = {pageaaai.pdf},
  selected = {true},
}

@article{jeongshin2022inrad,
    author = {Kyeong-Joong Jeong and Yong-Min Shin},
    title = {Time-Series Anomaly Detection with Implicit Neural Representation},
    journal = {arXiv preprint},
    volume = {},
    number = {},
    pages = {},
    year = {2022},
    doi = {https://doi.org/10.48550/arXiv.2201.11950},
    html = {https://arxiv.org/abs/2201.11950},
    selected = {true},
    abbr = {arXiv},
    pdf = {inrad.pdf},
}

@article{Shin2023edgelessgnn,
abbr = {TETC},
author = {Y. Shin and C. Tran and W. Shin and X. Cao},
journal = {IEEE Transactions on Emerging Topics in Computing},
title = {Edgeless-GNN: Unsupervised Representation Learning for Edgeless Nodes},
year = {2022},
volume = {},
number = {01},
issn = {2168-6750},
pages = {1-14},
abstract = {We study the problem of embedding edgeless nodes such as users who newly enter the underlying network, while using graph neural networks (GNNs) widely studied for effective representation learning of graphs. Our study is motivated by the fact that GNNs cannot be straightforwardly adopted for our problem since message passing to such edgeless nodes having no connections is impossible. To tackle this challenge, we propose Edgeless-GNN, a novel inductive framework that enables GNNs to generate node embeddings even for edgeless nodes through unsupervised learning. Specifically, we start by constructing a proxy graph based on the similarity of node attributes as the GNN&#x27;s computation graph defined by the underlying network. The known network structure is used to train model parameters, whereas a topology-aware loss function is established such that our model judiciously learns the network structure by encoding positive, negative, and second-order relations between nodes. For the edgeless nodes, we inductively infer embeddings by expanding the computation graph. By evaluating the performance of various downstream machine learning tasks, we empirically demonstrate that Edgeless-GNN exhibits (a) superiority over state-of-the-art inductive network embedding methods for edgeless nodes, (b) effectiveness of our topology-aware loss function, (c) robustness to incomplete node attributes, and (d) a linear scaling with the graph size.},
doi = {10.1109/TETC.2023.3292240},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {jul},
html = {https://www.computer.org/csdl/journal/ec/5555/01/10179257/1OH1E9ePmoM},
selected = {true},
pdf = {edgelessgnn.pdf},
}



@article{Moon2022explainablegait,
    doi = {10.1371/journal.pone.0264783},
    abbr = {PLOS ONE},
    author = {Moon, Jucheol AND Shin, Yong-Min AND Park, Jin-Duk AND Minaya, Nelson Hebert AND Shin, Won-Yong AND Choi, Sang-Il},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {Explainable gait recognition with prototyping encoder–decoder},
    year = {2022},
    month = {03},
    volume = {17},
    url = {https://doi.org/10.1371/journal.pone.0264783},
    pages = {1-20},
    abstract = {Human gait is a unique behavioral characteristic that can be used to recognize individuals. Collecting gait information widely by the means of wearable devices and recognizing people by the data has become a topic of research. While most prior studies collected gait information using inertial measurement units, we gather the data from 40 people using insoles, including pressure sensors, and precisely identify the gait phases from the long time series using the pressure data. In terms of recognizing people, there have been a few recent studies on neural network-based approaches for solving the open set gait recognition problem using wearable devices. Typically, these approaches determine decision boundaries in the latent space with a limited number of samples. Motivated by the fact that such methods are sensitive to the values of hyper-parameters, as our first contribution, we propose a new network model that is less sensitive to changes in the values using a new prototyping encoder–decoder network architecture. As our second contribution, to overcome the inherent limitations due to the lack of transparency and interpretability of neural networks, we propose a new module that enables us to analyze which part of the input is relevant to the overall recognition performance using explainable tools such as sensitivity analysis (SA) and layer-wise relevance propagation (LRP).},
    number = {3},
    selected = {true},
    pdf = {explainablegait.pdf},
    html = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0264783},
}

@inproceedings{Zhou2003labelprop,
  author       = {Dengyong Zhou and
                  Olivier Bousquet and
                  Thomas Navin Lal and
                  Jason Weston and
                  Bernhard Schölkopf},
  title        = {Learning with Local and Global Consistency},
  booktitle    = {NIPS},
  pages        = {321--328},
  address = {Vancouver and Whistler, British Columbia, Canada},
  month = {Dec.},
  year         = {2003},
  abbr = {NIPS},
  html = {https://proceedings.neurips.cc/paper/2003/hash/87682805257e619d49b8e0dfdc14affa-Abstract.html},
}

@inproceedings{Yang2016planetoid,
  author       = {Zhilin Yang and
                  William W. Cohen and
                  Ruslan Salakhutdinov},
  title        = {Revisiting Semi-Supervised Learning with Graph Embeddings},
  booktitle    = {{ICML} 2016},
  pages        = {40--48},
  year         = {2016},
  month = {Jun.},
  address = {New York City, NY},
  html = {http://proceedings.mlr.press/v48/yanga16.html},
}
