<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://jordan7186.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://jordan7186.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-12-14T13:12:08+00:00</updated><id>https://jordan7186.github.io/feed.xml</id><title type="html">Yong-Min Shin</title><subtitle>Welcome to my homepage! </subtitle><entry><title type="html">Revisiting Modern Benchmarks for Semi-suparvised Node Classification using Classical Methods</title><link href="https://jordan7186.github.io/blog/2022/Revisiting_SSL/" rel="alternate" type="text/html" title="Revisiting Modern Benchmarks for Semi-suparvised Node Classification using Classical Methods"/><published>2022-10-08T00:00:00+00:00</published><updated>2022-10-08T00:00:00+00:00</updated><id>https://jordan7186.github.io/blog/2022/Revisiting_SSL</id><content type="html" xml:base="https://jordan7186.github.io/blog/2022/Revisiting_SSL/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Semi-supervised learning (SSL) is one of the most widely studied tasks in machine learning. As the â€˜supervisedâ€™ part of the name suggessts, data instances are accompanied by its labels, which are sereved as the ground-truth. However, â€˜semiâ€™ suggests that not all given instance are learned by supervised learning, but rather a (small) portion of it also have access its ground-truth labels by the model. In the context of graph learning, the task of (semi-supervised) node classification is of the main concern for many research papers. The history to solve this task goes quite a way back, one of which is <d-cite key="Zhou2003labelprop"></d-cite>, published back in 2003, before the era of machine learning.</p> <p>In this post, I will revisit this classic method and apply to one of the more modern benchmark datasets, and try to see what analysis can be done to gain some insights.</p> <h2 id="setup--task">Setup &amp; Task</h2> <p>One of the main points to consider beforehand is the fact that [1] assumes that the given dataset is not a graph. Rather, it is a â€˜typicalâ€™ dataset where we are given a bunch of datapoints that can be described as a set of vectors with a fixed dimension, along with a set of class labels. So one of the main concern of the paper is actually the construction of a graph in which the main algorithm will run on. This graph basically constructs a connection (or an edge in graph terms) based on the pairwise similarity between two datapoints. Therefore, the structure of the resulting graph will highlight how much each data have common features in the context of the enitre dataset; a data with a lot of connections will likely to have information shared across many other datapoints.</p> <p>However, since we are interested in applying the main algorithm in modern <strong>graph benchmark datasets</strong>, we will skip the graph construction part altogether. Instead, we will use the <strong>given graph structure directly</strong>, and lets see how it performs.</p> <h2 id="main-idea-of-the-algorithm">Main Idea of the algorithm</h2> <p>The algorithm itself is first described as an iterative process. Going a bit formal, let us define some terms here:</p> <ul> <li>Denote $n$ as the number of datapoints (which is equal to the number of <strong>nodes</strong> if we interpret the dataset as a graph).</li> <li>As we deal with graph datasets, we can assume that the adjacency matrix $A \in \mathcal{R}^{n \times n}$ is given. (Separately constructed as an affinity matrix in [1])</li> <li>Assume each nodes are assigned to one of $c$ classes, which we express as a one-hot encoded vector.</li> <li>The matrix $\mathcal{Y} \in \mathcal{R}^{n \times c}$ represents the class information for all nodes in the dataset.</li> <li>The task is to generate a good prediction matrix $\mathcal{F} \in \mathbb{R}^{n \times c}$, where the actual predicted class is considered as the index of the highest value for each row: $y_i = \text{arg} \max_{j} \mathcal{F}_{ij}$.</li> </ul> <h3 id="commonly-used-assumption-in-ssl">Commonly used assumption in SSL</h3> <p>The iteration itself encompasses the philosophy of solving the problem of semi-supervised classification. Naturally, we would like to directly utilize the label information of the nodes that are available (Idea 1). Unfortunately, most of the nodes does not have such access. Semi-supervised (node) classification therefore adopts the following assumption about the dataset (task):</p> <aside> ðŸ’¡ Data points that are close/similar with each other will likely to have the same labels (Idea 2), </aside> <p>which tends to be effective for quite a lot of cases.</p> <h3 id="algorithm">Algorithm</h3> <p>The two ideas (direct loss and similarity assumption) are directly represented in the algorithm, where it can be written as an iterative procedure in the $t$-th step:</p> \[\mathcal{F}(t+1) = \alpha A \mathcal{F}(t) + (1-\alpha)\mathcal{Y}.\] <p>Dissecting each term, we can recover each of the ideas directly. Starting from the second term, $(1-\alpha)\mathcal{Y}$ basically just asserts the label information (at least the available ones) into $\mathcal{F}(t+1)$ (Idea 1). Now the first term, $\alpha A \mathcal{F}(t)$, basically reuse the sum (scaled down by $(1-\alpha)$) of what their neighbor says (Idea 2). Here, we implicitly defined the notion of â€˜similarâ€™ in the graph case: Connected nodes are close to each other. This assumption also works well for other graph laerning.</p> <h3 id="limit-case">Limit case</h3> <p>The nice thing about this formulation is that, we can directly calculate $\lim_{t \rightarrow \infty} \mathcal{F}(t)$ analytically. The math itself is also quite straightforward, and the asymptotic result is:</p> \[\lim_{t \rightarrow \infty} \mathcal{F}(t) = (I - \alpha S)^{-1} Y\] <h2 id="application-to-cora">Application to Cora</h2> <p>Cora is perhaps the most used graph benchmark dataset for evaluating on node or edge level tasks (popularized by <d-cite key="Yang2016planetoid"></d-cite>). As mentioned, we will only use the graph structure data as the algorithm does not accept node feature as input.</p> <p>We will use <a href="https://pytorch.org/">pytorch</a> and <a href="https://pytorch-geometric.readthedocs.io/">pytorch geometric</a> as our base framework.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">os</span>
<span class="kn">from</span> <span class="n">torch.nn</span> <span class="kn">import</span> <span class="n">functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">from</span> <span class="n">torch_geometric.datasets</span> <span class="kn">import</span> <span class="n">Planetoid</span>
<span class="kn">from</span> <span class="n">torch_geometric.utils</span> <span class="kn">import</span> <span class="n">to_dense_adj</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="n">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>

<span class="n">benchmark</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Cora</span><span class="sh">"</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="nc">Planetoid</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="sh">""</span><span class="p">.</span><span class="nf">join</span><span class="p">([</span><span class="n">os</span><span class="p">.</span><span class="nf">getcwd</span><span class="p">(),</span> <span class="sh">"</span><span class="s">/Cora</span><span class="sh">"</span><span class="p">]),</span> <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">Cora</span><span class="sh">"</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span><span class="p">)</span>

<span class="n">data</span><span class="p">.</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nc">FloatTensor</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">x</span><span class="p">)</span>
<span class="n">onehot_label</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">one_hot</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">y</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">dataset</span><span class="p">.</span><span class="n">num_classes</span><span class="p">)</span>
<span class="n">onehot_label</span> <span class="o">=</span> <span class="n">onehot_label</span><span class="p">.</span><span class="nf">float</span><span class="p">()</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">x</span><span class="p">[</span><span class="n">data</span><span class="p">.</span><span class="n">train_mask</span><span class="p">]</span>  <span class="c1"># feature matrix
</span><span class="n">y_train</span> <span class="o">=</span> <span class="n">onehot_label</span><span class="p">[</span><span class="n">data</span><span class="p">.</span><span class="n">train_mask</span><span class="p">]</span>  <span class="c1"># node labels</span></code></pre></figure> <h3 id="implementation-1-explicit-iteration">Implementation 1: Explicit iteration</h3> <p>As a first step, we process the data more fit to the algorithm.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">A</span> <span class="o">=</span> <span class="nf">to_dense_adj</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">edge_index</span><span class="p">).</span><span class="nf">squeeze_</span><span class="p">()</span>
<span class="n">D_half</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">diag</span><span class="p">((</span><span class="n">A</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">).</span><span class="nf">squeeze_</span><span class="p">())</span> <span class="o">**</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span> <span class="o">/</span> <span class="mi">2</span><span class="p">),</span> <span class="n">diagonal</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">S</span> <span class="o">=</span> <span class="n">D_half</span> <span class="o">@</span> <span class="n">A</span> <span class="o">@</span> <span class="n">D_half</span> <span class="c1"># Normalization according to the paper
</span><span class="n">blank</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nc">Tensor</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">])</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">onehot_label</span><span class="p">)</span> <span class="c1"># This is the actual data
</span><span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">onehot_label</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
    <span class="k">if</span> <span class="n">data</span><span class="p">.</span><span class="n">train_mask</span><span class="p">[</span><span class="n">row</span><span class="p">]:</span>
        <span class="n">Y</span><span class="p">[</span><span class="n">row</span><span class="p">]</span> <span class="o">=</span> <span class="n">onehot_label</span><span class="p">[</span><span class="n">row</span><span class="p">]</span> <span class="c1"># Masking is needed to ensure only using training data</span></code></pre></figure> <p>The implementation can be straightforwardly done by following the paper:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># Custom function to measure the accuracy
</span><span class="k">def</span> <span class="nf">calculate_accuracy</span><span class="p">(</span><span class="n">raw_prediction</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
    <span class="n">indexed_prediction</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">raw_prediction</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">indexed_target</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="k">if</span> <span class="n">target</span><span class="p">.</span><span class="nf">dim</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">target</span>
    <span class="k">return</span> <span class="nf">sum</span><span class="p">(</span><span class="n">indexed_prediction</span> <span class="o">==</span> <span class="n">indexed_target</span><span class="p">).</span><span class="nf">item</span><span class="p">()</span> <span class="o">/</span> <span class="nf">len</span><span class="p">(</span><span class="n">indexed_prediction</span><span class="p">)</span>

<span class="c1"># Explicitly define one iteration of the algorithm
</span><span class="k">def</span> <span class="nf">run_one_iteration</span><span class="p">(</span><span class="n">F</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">mm</span><span class="p">(</span><span class="n">S</span><span class="p">,</span> <span class="n">F</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">)</span> <span class="o">*</span> <span class="n">Y</span>

<span class="c1"># Run the whole iteration for a given tot_iteration
</span><span class="k">def</span> <span class="nf">run_explicit_algorithm</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">tot_iteration</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span>
    <span class="n">acc_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">F</span> <span class="o">=</span> <span class="n">Y</span>  <span class="c1"># initial iteration
</span>    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">tot_iteration</span><span class="p">),</span> <span class="n">desc</span><span class="o">=</span><span class="sa">f</span><span class="sh">"</span><span class="s">Alpha: </span><span class="si">{</span><span class="n">alpha</span><span class="si">:</span><span class="p">.</span><span class="mi">1</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">):</span>
        <span class="n">prediction</span> <span class="o">=</span> <span class="nf">run_one_iteration</span><span class="p">(</span><span class="n">F</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
        <span class="n">accuracy</span> <span class="o">=</span> <span class="nf">calculate_accuracy</span><span class="p">(</span><span class="n">prediction</span><span class="p">[</span><span class="n">mask</span><span class="p">,</span> <span class="p">:],</span> <span class="n">target</span><span class="p">[</span><span class="n">mask</span><span class="p">])</span>
        <span class="n">acc_list</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">accuracy</span><span class="p">)</span>
        <span class="n">F</span> <span class="o">=</span> <span class="n">prediction</span>  <span class="c1"># for the next iteration
</span>    <span class="k">return</span> <span class="n">acc_list</span>

<span class="c1"># Run multiple sessions by varying alpha
</span><span class="k">def</span> <span class="nf">experiment_with_alpha</span><span class="p">(</span><span class="n">S</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">tot_iteration</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span>
    <span class="n">alpha_list</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">9</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span>
        <span class="nf">run_explicit_algorithm</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">tot_iteration</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">alpha</span> <span class="ow">in</span> <span class="n">alpha_list</span>
    <span class="p">]</span>

<span class="c1"># The results for the whole session is saved in result
</span><span class="n">tot_iteration</span> <span class="o">=</span> <span class="mi">16</span>
<span class="n">result</span> <span class="o">=</span> <span class="nf">experiment_with_alpha</span><span class="p">(</span>
    <span class="n">S</span><span class="o">=</span><span class="n">S</span><span class="p">,</span>
    <span class="n">Y</span><span class="o">=</span><span class="n">Y</span><span class="p">,</span>
    <span class="n">tot_iteration</span><span class="o">=</span><span class="n">tot_iteration</span><span class="p">,</span>
    <span class="n">target</span><span class="o">=</span><span class="n">data</span><span class="p">.</span><span class="n">y</span><span class="p">,</span>
    <span class="n">mask</span><span class="o">=~</span><span class="n">data</span><span class="p">.</span><span class="n">train_mask</span><span class="p">,</span>
<span class="p">)</span></code></pre></figure> <p>Here, the performance is measured by accuracy (for the test nodes of course).</p> <p>Plotting all experiment sessions in which we perform experiments with different values of alpha:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog1_1-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog1_1-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog1_1-1400.webp"/> <img src="/assets/img/blog1_1.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Plot of performance vs. iteration, while varying the value of alpha. </div> <p>According to this result, there are some interesting characteristics that we can observe:</p> <ul> <li>The iteration converges quite fase, and we need ~10 iterations to achieve performance near convergence. We will see this more directly in the next experiment.</li> <li>The effect of alpha is quite consistent accros all values: As alpha increases, the model achievec higher performance. The interpretation of this effect is also strightforward: The neighbor information is quite helpful for solving node classification task in Cora, and the given graph structure is nicely aligned with the â€˜closenessâ€™ that we discussed earlier. We will also see this effect again in the next experiment.</li> </ul> <h2 id="implementation-2-analytic-limit-solution">Implementation 2: Analytic limit solution</h2> <p>Now, we will implement the solution by taking the iteration $t$ to $\infty$. This is even more straightfoward:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># Direct calculation of limit solution
</span><span class="k">def</span> <span class="nf">get_prediction_limit</span><span class="p">(</span><span class="n">S</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">inverse</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">eye</span><span class="p">(</span><span class="n">W</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">S</span><span class="p">)</span> <span class="o">@</span> <span class="n">Y</span>


<span class="c1"># Get the performance of the experiment
</span><span class="k">def</span> <span class="nf">perform_experiment_limit</span><span class="p">(</span><span class="n">S</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
    <span class="n">raw_prediction</span> <span class="o">=</span> <span class="nf">get_prediction_limit</span><span class="p">(</span><span class="n">S</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>
    <span class="k">return</span> <span class="nf">calculate_accuracy</span><span class="p">(</span><span class="n">raw_prediction</span><span class="p">[</span><span class="n">mask</span><span class="p">,</span> <span class="p">:],</span> <span class="n">target</span><span class="p">[</span><span class="n">mask</span><span class="p">])</span>

<span class="c1"># The experiment is done with various alpha values
</span><span class="n">performace_list_limit</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">alpha_list</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">9</span><span class="p">)</span>
<span class="k">for</span> <span class="n">alpha</span> <span class="ow">in</span> <span class="n">alpha_list</span><span class="p">:</span>
    <span class="n">result</span> <span class="o">=</span> <span class="nf">perform_experiment_limit</span><span class="p">(</span>
        <span class="n">S</span><span class="o">=</span><span class="n">S</span><span class="p">,</span> <span class="n">Y</span><span class="o">=</span><span class="n">Y</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="n">data</span><span class="p">.</span><span class="n">y</span><span class="p">,</span> <span class="n">mask</span><span class="o">=~</span><span class="n">data</span><span class="p">.</span><span class="n">train_mask</span>
    <span class="p">)</span>
    <span class="n">performace_list_limit</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">result</span><span class="p">)</span></code></pre></figure> <p>Again, plotting all results of the limit solution version as a plot results in:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog1_2-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog1_2-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog1_2-1400.webp"/> <img src="/assets/img/blog1_2.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Plot of performance against different values of alpha. The performane of MLP is also shown as the dotted line. </div> <p>From this plot, we also observe some characteristics:</p> <ul> <li>The effect of alpha is consisent with that of Figure 1.</li> <li>The plot also shows the performance of only using the feature information of Cora by training a two-layer MLP model. Although the performance is not that bad (considering only using a part of the model + simple model), [1] still outperforms. This implies that the neighbor information is more vital than the node feature information.</li> </ul> <p>Additionally, we can plot the asymtotic behavior of the iteative algorithm:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog1_3-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog1_3-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog1_3-1400.webp"/> <img src="/assets/img/blog1_3.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Plot of the performance vs. iteration, compared between the iterative solution and the analytic in the limit. </div> <p>As mentioned, the iteration converges quite fast, and achieves the optimal performance near ~10 iterations. However, we need furhter experimental evaluations to see whether this fast convergence is due to the dataset or the algorithm.</p> <h2 id="conclusion">Conclusion</h2> <p>In this post, we have revisited the task of semi-supervised node classification. The frequently used benchmark dataset for modern graph learning models, Cora, was tested with a more classical (but insightful) matrix iterative method. Although the algorithm only utilizes the graph information and does not use node feature information, we have seen that the neighbor defined as the graph structure is more critical in soving the task. Perhaps extending to other datasets might reveal to what extent each parts of the dataset (feature vs. structure) affects the performace.</p>]]></content><author><name></name></author><category term="ClassicalMethods"/><category term="Graph_learning"/><summary type="html"><![CDATA[Introduction]]></summary></entry></feed>