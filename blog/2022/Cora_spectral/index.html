<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Dissecting Cora in the Spectral Domain | Yong-Min Shin</title> <meta name="author" content="Yong-Min Shin"> <meta name="description" content="Graph signal processing and Fourier analysis on the Cora dataset"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="preconnect" href="https://fonts.googleapis.com"> <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin> <link href="https://fonts.googleapis.com/css2?family=Atkinson+Hyperlegible:ital,wght@0,400;0,700;1,400;1,700&amp;family=JetBrains+Mono:wght@400;500;600&amp;display=swap" rel="stylesheet"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A4%94&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://jordan7186.github.io/blog/2022/Cora_spectral/"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand" href="/"><span class="brand-name">Yong-Min Shin</span></a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/presentations/">Presentations</a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container-fluid px-md-5 mt-5"> <article class="post-article"> <header class="post-header"> <h1 class="post-title">Dissecting Cora in the Spectral Domain</h1> <p class="post-subtitle">Graph signal processing and Fourier analysis on the Cora dataset</p> <div class="post-meta-line"> <time datetime="2022-10-19T00:00:00+00:00">October 19, 2022</time> <span class="meta-separator">Â·</span> <span class="read-time">17 min read</span> </div> </header> <div class="series-nav"> <div class="series-nav-header"> <span class="series-label">ðŸ“š Part 1 of 2</span> <span class="series-title">Graph Signal Processing</span> </div> <div class="series-nav-links"> <div class="series-nav-link series-nav-placeholder"></div> <a href="/blog/2022/ChebConv/" class="series-nav-link series-nav-next"> <span class="series-nav-direction">Next â†’</span> <span class="series-nav-part-title">Chebyshev Spectral Convolution</span> </a> </div> </div> <div class="post-content prose"> <h1 id="introduction">Introduction</h1> <p>In graph signal processing (GSP), graph data is analyzed in the Fourier domain. Although Fourier analysis itself is already well-known on other types of data, extending the Fourier transform for analysis on <strong>graph data</strong> is quite interesting and worth diving into.</p> <p>This post concerns the basic idea of GSP and further investigates its application with the Cora dataset.</p> <h1 id="notations">Notations</h1> <p>For ease of description, let us define some notations and assumptions before moving on.</p> <h2 id="basic-graph-notations">Basic graph notations</h2> <ul> <li>Let us assume a graph \(G=(V, E)\) is given, where \(V\) is the set of nodes and \(E\).</li> <li>The number of nodes in the graph is \(n\).</li> <li>It makes life simple if we think of each node \(i \in V\) also as natural numbers (\(i = 1, \cdots, n\)).</li> <li>The adjacency matrix is denoted as \(A\), and the degree matrix as \(D = diag(d_1, \cdots, d_n)\) where \(d_i = \sum_{j=1}^n A_{ij}\).</li> </ul> <h2 id="graph-signals">Graph signals</h2> <ul> <li>For each node \(i \in V\), a single scalar is assigned as the signal of the node. Its basically the same as defining the node feature matrix, where the dimension of the features is one.</li> <li>The graph signal is represented as a \(n\)-dimensional vector \({\bf g} \in \mathbb{R}^{n}\). Making our notations pytorch-like, the signal for node \(i\) is denoted as \({\bf g}[i] \in \mathbb{R}\) (But here \(i\) starts at 1, not zero).</li> </ul> <h1 id="extension-of-fourier-transform-to-graphs">Extension of Fourier transform to graphs</h1> <h3 id="1d-fourier-transform">1D Fourier transform</h3> <p>Fourier transforms enables us to view the given data from the original domain (such as time) to the frequency domain. To apply such transformations to graph signals (data defined on graphs), we need to first generalize the Fourier transformation itself. This insight is nicely described in <span class="citation" data-preview="Shuman, D. I., Narang, S. K., Frossard, P., Ortega, A., &amp; Vandergheynst, P. (2013). The Emerging Field of Signal Processing on Graphs. IEEE Signal Processing Magazine.">(Shuman et al., 2013)</span>, which we will follow also.</p> <p>The most frequent <del>(no pun intended)</del> equation that you might have encountered when looking up for Fourier transformation would be the following:</p> \[\hat{f}(\zeta) = \int_{\mathbb{R}} f(t) e^{-2 \pi i \zeta t} dt,\] <p>where the function \(f\) defined on the (probably) time domain (i.e., \(t\)) is transformed into a function \(\hat{f}\) defined on the frequency domain (i.e., \(\zeta\)). The interpretation of the integral itself is beautifully described by 3Blue1Brown (<a href="https://www.youtube.com/watch?v=spUNpyF58BY" rel="external nofollow noopener" target="_blank">link</a>).</p> <p>In more general terms, the Fourier transformation can be thought of as the inner product between the original function and the eigenfunctions of the Laplace operator. Concretely,</p> \[\hat{f}(\zeta) = &lt;f, e^{2\pi i\zeta i}&gt;,\] <p>and you can see the first equation naturally aligns with the general definition of Fourier transform. Eq. (2) in (Shuman et al., 2013) also shows that \(e^{2\pi i \zeta i}\) is indeed the eigenfunction of the Laplace operator in 1D.</p> <h3 id="graph-fourier-transform">Graph fourier transform</h3> <p>This generalized definition provides the way to extend the Fourier transformation to graph data. By reminding that the graph data is a discrete data that are represented as vectors rather than functions, the extension of Fourier transform is now clear : 1) Get the eigen<strong>vectors</strong> of the Laplacian <strong>matrix</strong> 2) Inner product with the original graph signal.</p> <p>More concretely, the transformed graph signal is described as:</p> \[\hat{\mathbf{g}}[l] = &lt;{\bf g}, {\bf u}_l&gt;,\] <p>where \({\bf u}_l\) is the eigvenvector corresponding to the \(l\)-th lowest eigenvalue of the Laplacian matrix (\(l = 1, \cdots, n\)). Notice that the domain has shifted from \(i\) to \(l\).</p> <p>This implies several things:</p> <ul> <li>The Laplacian matrix is conceptually equivalent as the Laplace operator: (Shuman et al., 2013) provides an intuitive explanation by introducing concepts from discrete calculus.</li> <li>We need to perform eigendecomposition for the Laplacian matrix for this to happen: We need the eigenvectors \({\bf u}_l\).</li> <li>The inner product is now vector multiplication between vectors:</li> </ul> \[\hat{\mathbf{g}}[l] = {\bf u}_l^{T}{\bf g}\] <ul> <li>Or, in vector form:</li> </ul> \[\hat{\mathbf{g}} = U^T \mathbf{g}\] <h2 id="eigendecomposing-sbms">Eigendecomposing SBMs</h2> <p>To see what the eigenvectors of the graph Laplacian is, lets directly compute such vectors by using an intuitive example. Perhaps the best example to illustrate is stochastic block models (SBMs), which is a graph generative model where the user can control simple community structures genertated from the model.</p> <p>Two pieces of information is required for the SBM model to generate graphs with \(c\) number of communities: A vector that defines the number of nodes to be included in each community, and a symmetric matrix indicating the edge probability between (or within) communities.</p> <p>The code below shows one example of an SBM model generated by pytorch geometric, with the help of the networkx package for visualization:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># Generate SBM graph with Pytorch Geometric
</span><span class="kn">import</span> <span class="n">torch_geometric</span> <span class="k">as</span> <span class="n">pyg</span>
<span class="kn">from</span> <span class="n">torch_geometric.data</span> <span class="kn">import</span> <span class="n">Data</span>
<span class="kn">from</span> <span class="n">torch_geometric.utils</span> <span class="kn">import</span> <span class="n">stochastic_blockmodel_graph</span> <span class="k">as</span> <span class="n">sbm</span>
<span class="kn">from</span> <span class="n">torch_geometric.utils</span> <span class="kn">import</span> <span class="n">to_networkx</span>
<span class="kn">import</span> <span class="n">networkx</span> <span class="k">as</span> <span class="n">nx</span>

<span class="n">block_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="mi">40</span><span class="p">,</span> <span class="mi">60</span><span class="p">]</span>  <span class="c1"># Two communities
</span><span class="n">node_color</span> <span class="o">=</span> <span class="p">[</span><span class="sh">"</span><span class="s">red</span><span class="sh">"</span><span class="p">]</span> <span class="o">*</span> <span class="mi">40</span> <span class="o">+</span> <span class="p">[</span><span class="sh">"</span><span class="s">blue</span><span class="sh">"</span><span class="p">]</span> <span class="o">*</span> <span class="mi">60</span>  <span class="c1"># Set node colors for drawing
</span><span class="n">edge_probs</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.005</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.005</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">]]</span>  <span class="c1"># Symmetric since our graph is undirected
</span>
<span class="c1"># Graph data in PyG is represented as a torch_geometric.data.Data object
</span><span class="n">sbm_torch</span> <span class="o">=</span> <span class="nc">Data</span><span class="p">(</span>
    <span class="n">edge_index</span><span class="o">=</span><span class="nf">sbm</span><span class="p">(</span><span class="n">block_sizes</span><span class="o">=</span><span class="n">block_sizes</span><span class="p">,</span> <span class="n">edge_probs</span><span class="o">=</span><span class="n">edge_probs</span><span class="p">),</span>
    <span class="n">num_nodes</span><span class="o">=</span><span class="nf">sum</span><span class="p">(</span><span class="n">block_sizes</span><span class="p">),</span>
<span class="p">)</span>

<span class="c1"># To visualize, move from torch_geometric.data.Data to networkx.Graph object
</span><span class="n">sbm_torch</span> <span class="o">=</span> <span class="nf">to_networkx</span><span class="p">(</span><span class="n">sbm_torch</span><span class="p">,</span> <span class="n">to_undirected</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">nx</span><span class="p">.</span><span class="nf">draw</span><span class="p">(</span><span class="n">sbm_torch</span><span class="p">,</span> <span class="n">node_size</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">node_color</span><span class="o">=</span><span class="n">node_color</span><span class="p">)</span></code></pre></figure> <figure class="figure-numbered"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog2_1-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog2_1-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog2_1-1400.webp"></source> <img src="/assets/img/blog2_1.png" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <figcaption>A graph generated from SBM using the code above.</figcaption> </figure> <p>In Figure 1, the probability of an edge existing between different communities (color coded in blue and red, respectively) is set to 0.005, while the probability within the same community is set to 0.2. Note that the edges are independently generated. If we set the probability between different community to zero, graphs such as in Figure 2 will be generated.</p> <p>We will use the following quite dramatic but managable graph for decomposition:</p> <figure class="figure-numbered"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog2_2-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog2_2-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog2_2-1400.webp"></source> <img src="/assets/img/blog2_2.png" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <figcaption>The graph to be decomposed. It contains 10 nodes in each connected components.</figcaption> </figure> <p>The eigendecomposition of the unnormalized graph Laplacian \(L = D-A\) is easilty computed by modern libraries, such as numpy or pytorch. The values of the sorted eigenvalues are as follows:</p> <figure class="figure-numbered"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog2_3-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog2_3-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog2_3-1400.webp"></source> <img src="/assets/img/blog2_3.png" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <figcaption>Plot of the eigenvalues, sorted in increasing order.</figcaption> </figure> <p>We can clearly see that three eigenvalues are at zero, indicating that there are three very distinct community structures (i.e., connected components) within the graph structure. The smallest eigenvalues (frequencies) correspond to number of connected components, since <strong>connected components is the most â€˜macroâ€™ view when looking at the graph structure</strong>. This is analogous to the <a href="https://en.wikipedia.org/wiki/Fourier_series#Sine-cosine_form" rel="external nofollow noopener" target="_blank">Fourier series expansion</a> of a periodic function. Roughly speaking, the Fourier series expansion can be thought of re-expressing a function with sine and cosine basis:</p> \[f(x) = a_0 + \sum_{n}^{\infty}(a_n \sin (\omega_nx) + b_n \cos (\omega_nx)),\] <p>and the â€˜lowest frequencyâ€™ sine and cosines inside the summation have the â€˜widestâ€™ valleys (or the â€˜macroâ€™ view since they are responsible of describing the large-scale characteristics).</p> <p>We can them anticipate the eigenvalues when we slightly modify the graph (see Figure 4):</p> <figure class="figure-numbered"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog2_4-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog2_4-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog2_4-1400.webp"></source> <img src="/assets/img/blog2_4.png" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <figcaption>Modified graph, now the whole graph is connected.</figcaption> </figure> <figure class="figure-numbered"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog2_5-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog2_5-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog2_5-1400.webp"></source> <img src="/assets/img/blog2_5.png" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <figcaption>Plot of the sorted eigenvalues for the modified graph.</figcaption> </figure> <p>Now, the number of zero eigenvalues in Figure 6 becomes one, since the graph now has one connected component. However we can still see that the second and third lowest values are still quite near zero.</p> <p>It is also worth illustrating the eigenvectors itself. Following conventional notiations, lets say that the Laplacian matrix \(L\) is decomposed into \(L = U \Lambda U^{T}\), where the diagonal matirx \(\Lambda\) contains the (ordered) eigenvalues and \(U\) contains the eigenvectors:</p> \[U = \begin{bmatrix} \vert &amp; &amp;\vert \\ {\bf u}_1 &amp; \cdots &amp; {\bf u}_{n} \\ \vert &amp; &amp;\vert \end{bmatrix}\] <p>Lets straight up visualize \(U\).</p> <figure class="figure-numbered"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog2_6-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog2_6-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog2_6-1400.webp"></source> <img src="/assets/img/blog2_6.png" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <figcaption>Heatmap visualization of the eigenvectors (also sorted by their corresponding eigenvalues in ascending order).</figcaption> </figure> <p>Here, we can see the different modes of the graph signals, which are combined together to form the original graph signals. In the left-most column (\({\bf u}_1\)), the values all have the same values. This eigenvector corresponds to the lowest eigenvalue (which is zero), and the same values indicate the â€˜one connected componentâ€™ information.</p> <p>However, the eigenvector right next to it (\({\bf u}_2\), corresponding to the second-lowest eigenvalue) still shows the three community structures present in the graph. To be more explicit, lets plot \({\bf u}_1\):</p> <figure class="figure-numbered"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog2_7-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog2_7-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog2_7-1400.webp"></source> <img src="/assets/img/blog2_7.png" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <figcaption>Direct plot.</figcaption> </figure> <p>In short, assuming that the graph has only one connected component, \({\bf u}_1\) encodes the community structure of the graph. This is actually the main idea behind <strong>spectral clustering</strong> (Luxberg, 2007).</p> <h1 id="low-pass-filtering-cora">Low-pass filtering Cora</h1> <p>Regarding graph signal processing and Fourier transform, the paper <span class="citation" data-preview="NT, H., &amp; Maehara, T. (2019). Revisiting Graph Neural Networks: All We Have is Low-Pass Filters. arXiv preprint.">(NT &amp; Maehara, 2019)</span> introduces a straightforward experiment on the Cora dataset <span class="citation" data-preview="Yang, Z., Cohen, W., &amp; Salakhutdinov, R. (2016). Revisiting Semi-Supervised Learning with Graph Embeddings. ICML.">(Yang et al., 2016)</span> (Specifically, section 3).</p> <h3 id="the-experiment-procedure-in-nt--maehara-2019">The experiment procedure in (NT &amp; Maehara, 2019)</h3> <p>Denoting the feature matrix of Cora as \(X \in \mathbb{R}^{2703 \times 1433}\) and given an index \(k \in [1, 2703]\), this is the experiment procedure:</p> <ol> <li>Compute the Fourier basis \(U\) by eigendecomposing the graph Laplacian.</li> <li>Add Gaussian noise to the feature matrix: \(X \leftarrow X + \mathcal{N}(0, \sigma^2)\)</li> <li>Compute the lowest \(k\)-frequency component: \(X_k \leftarrow U[:k]^T X\)</li> <li>Reconstruct the features: \(\tilde{X}_k = U[:k] X_k\)</li> <li>Train an MLP on \(\tilde{X}_k\) for semi-supervised node classification.</li> </ol> <p>We attempt to reproduce the results and make some discussions and interpretations. In our case, we use the unnormalized graph Laplacian \(L = D-A\) as opposed to the symmetrically normalized version \(L_{norm} = D^{-1/2}AD^{-1/2}\). We also discard step 2 since we are not interested in robustness to noise.</p> <h3 id="in-theory">In theory</h3> <p>In theory, this is the same as <em>low-pass filtering</em> on the feature matrix \(X\). Rewriting in terms of (Shuman et al., 2013) (Look at Section III-A for more details), the procedure is the same as defining a filtering function \({\bf h}\), where in</p> \[{\bf h}(L) = U \begin{bmatrix} h(\lambda_1) &amp; &amp; {\bf 0}\\ &amp; \ddots &amp; \\ {\bf 0} &amp; &amp; h(\lambda_n) \end{bmatrix} U^T,\] <p>the function \(h\) is defind as:</p> \[h(\lambda_i) = \begin{cases} 1 &amp; \text{if}\ i \leq k \\ 0, &amp; \text{otherwise} \end{cases}.\] <p>In summary, the whole process can be described as</p> \[\mathbf{h}(L)X = U \mathbf{h}(\Lambda)U^T X,\] <p>where it transforms the feature matrix \(X\) to the frequency domain (\(U^T X\)), modify the signal by the function \(h\) (\(\mathbf{h}(\Lambda)U^T X\)), and return to the original domain (\(U \mathbf{h}(\Lambda)U^T X\)).</p> <p>Focusing on \(h(\lambda_i)\), it basically cuts off higher frequency signals (remind that the frequencies are determined by the graph structure) where the cutoff is at the \(k\)-th lowest frequency value. It just leaves the Fourier basis that are equal or below the \(k\)-th frequency.</p> <h3 id="our-experiment">Our experiment</h3> <p>This is the experimental result to reprocude (NT &amp; Maehara, 2019), where</p> <ul> <li>A 2 layer MLP was trained with early stopping.</li> <li>Same semi-supervised node classification setting as in (Yang et al., 2016).</li> <li>10 independent trials, plotting the average values in a solid line, alongside the one standard deviation.</li> <li>We plot the results alongside with the performance of an MLP with the same configurations using the original feature matrix \(X\) (pink dotted line).</li> <li>UL stands for unnormalized Laplacian (red line), NL stands for symmetrically normalized Laplacian (yellow line)</li> </ul> <figure class="figure-numbered"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Blog2_8-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Blog2_8-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Blog2_8-1400.webp"></source> <img src="/assets/img/Blog2_8.png" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <figcaption>Experimental results for the Cora dataset.</figcaption> </figure> <p>So, there are several things that are noticeable here:</p> <ol> <li>As expected, the performance is not so great at the extreme low end: There are simply not enough information for the MLP to get going.</li> <li>The <strong>peak performance</strong> happens near \(k\approx400\), even exceeding the case with using the original features. The value \(400\) is still quite a low value, considering that \(k\) can be up to 2703: This implies that the dataset itself benefits from low-frequency structure information. Such tendency aligns with the previous blog post, where assuming that nodes that are connected will probabily have similar labels helps solve node classification on Cora. Remind that locally, low-frequency means that the values between connected nodes does not change much.</li> <li>However, it seems quite weird that the performance does not seem to exactly recover the pink dotted line as we still continue to increase \(k\) near 2703. This issue will probably be linked with the computation error during the eigendecomposition. To show this, we directly calculate the difference bewteen \(X\) and \(U U^T X\): In theory, this is 0. In code, sadly, this is not the case:</li> </ol> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">from</span> <span class="n">torch.linalg</span> <span class="kn">import</span> <span class="n">eig</span>

<span class="n">L</span><span class="p">,</span> <span class="n">U</span> <span class="o">=</span> <span class="nf">eig</span><span class="p">(</span><span class="n">laplace</span><span class="p">)</span>
<span class="n">L_norm</span><span class="p">,</span> <span class="n">U_norm</span> <span class="o">=</span> <span class="nf">eig</span><span class="p">(</span><span class="n">laplace_norm</span><span class="p">)</span>

<span class="c1"># torch.dist: https://pytorch.org/docs/stable/generated/torch.dist.html
</span><span class="n">torch</span><span class="p">.</span><span class="nf">dist</span><span class="p">((</span><span class="n">U</span> <span class="o">@</span> <span class="n">U</span><span class="p">.</span><span class="n">T</span><span class="p">).</span><span class="n">real</span> <span class="o">@</span> <span class="n">data</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="n">data</span><span class="p">.</span><span class="n">x</span><span class="p">)</span>

<span class="o">&gt;&gt;</span> <span class="nf">tensor</span><span class="p">(</span><span class="mf">114.4097</span><span class="p">)</span>

<span class="n">torch</span><span class="p">.</span><span class="nf">dist</span><span class="p">((</span><span class="n">U_norm</span> <span class="o">@</span> <span class="n">U_norm</span><span class="p">.</span><span class="n">T</span><span class="p">).</span><span class="n">real</span> <span class="o">@</span> <span class="n">data</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="n">data</span><span class="p">.</span><span class="n">x</span><span class="p">)</span>

<span class="o">&gt;&gt;</span> <span class="nf">tensor</span><span class="p">(</span><span class="mf">101.5464</span><span class="p">)</span></code></pre></figure> <h1 id="conclusion">Conclusion</h1> <p>In this blog post, we have looked at the extension of Fouirer transformation in graph data as the core idea of graph signal processing. The re-representation of the graph in the frequency doman has quite straighforward interpretations, and we have also seen that the basis corresponding to the low frequency directly shows the macro-level structure. Expanding on the interpretation, we reprocude the experimental result in (NT &amp; Maehara, 2019), which directly show that the low-frequency features are vital in the semi-supervised classification for Cora.</p> <h1 id="appendix">Appendix</h1> <p>This is just out of curiosity, how about \(\mathcal{L}X\) instead of \(X\)?</p> <p>Here, the filter function is defined as</p> \[h'(\lambda_i) = \begin{cases} \lambda_i, &amp; \text{if}\ i \leq k \\ 0, &amp; \text{otherwise} \end{cases},\] <p>where in this version the filter actually preserves the frequency of the Laplacian.</p> <p>For brevity, we label different configurations as follows:</p> <table> <thead> <tr> <th>Â </th> <th>Unnormalized Laplacian</th> <th>Sym. norm. Laplacian</th> </tr> </thead> <tbody> <tr> <td>\(h\)</td> <td>UL (low pass)</td> <td>NL (low pass)</td> </tr> <tr> <td>\(h\)â€™</td> <td>UL</td> <td>NL</td> </tr> </tbody> </table> <p>The expanded experimental results are as follows:</p> <figure class="figure-numbered"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog2_9-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog2_9-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog2_9-1400.webp"></source> <img src="/assets/img/blog2_9.png" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <figcaption>Experimental results for the Cora dataset.</figcaption> </figure> <ul> <li>Overall, the performance degenerates when we use some form of \(\mathcal{L}X\) instead of \(X\), although the tendency for different \(k\) still remains the same. Considering the unnnormalized Laplaican, \(\mathcal{L} = D-A\), the minus sign in front of the adjacency matrix suggest that we are not actually aggregating the neighbor information; rather, we are emphasizing the difference. It shows that this is a poor representation of \(X\) to solve this task.</li> <li>For the case where we use the symmetrically normalized Laplacian matrix, it is the only configuration that shows that the performace increases as we add more high frequency information (blue line). There can be several explanations for this: <ul> <li>The high frequency information does have performance benefits for some limited cases</li> <li>The computation error from the eigendecomposition is less dominant and is therefore the reason of the performance increase</li> </ul> <p>, which will be an interesting topic for future investigation.</p> </li> </ul> </div> <section class="post-references"> <h2>References</h2> <ol class="reference-list"> <li> Yang, Z., Cohen, W., &amp; Salakhutdinov, R. (2016). <a href="https://arxiv.org/abs/1603.08861" rel="external nofollow noopener" target="_blank">Revisiting Semi-Supervised Learning with Graph Embeddings</a>. ICML. </li> <li> Shuman, D. I., Narang, S. K., Frossard, P., Ortega, A., &amp; Vandergheynst, P. (2013). <a href="https://arxiv.org/abs/1211.0053" rel="external nofollow noopener" target="_blank">The Emerging Field of Signal Processing on Graphs</a>. IEEE Signal Processing Magazine. </li> <li> Von Luxburg, U. (2007). <a href="https://arxiv.org/abs/0711.0189" rel="external nofollow noopener" target="_blank">A Tutorial on Spectral Clustering</a>. Statistics and Computing. </li> <li> NT, H., &amp; Maehara, T. (2019). <a href="https://arxiv.org/abs/1905.09550" rel="external nofollow noopener" target="_blank">Revisiting Graph Neural Networks: All We Have is Low-Pass Filters</a>. arXiv preprint. </li> </ol> </section> <section class="related-posts"> <h2>Other Posts</h2> <ul class="related-posts-list"> <li class="related-post-item"> <a href="/blog/2022/ChebConv/">Understanding ChebConv with the NEIGHBORSMATCH problem</a> <p>Deep dive into Chebyshev spectral graph convolutions and analysis using the NEIGHBORSMATCH problem</p> </li> <li class="related-post-item"> <a href="/blog/2022/Revisiting_SSL/">Revisiting Modern Benchmarks for Semi-suparvised Node Classification using Classical Methods</a> <p>Applying classical label propagation methods to modern graph benchmark datasets</p> </li> <li class="related-post-item"> <a href="/blog/2024/Fidelity_pruning/">On the feasibility of fidelity- for graph pruning</a> <p>Exploring whether GNN explanation methods can be repurposed for graph pruning</p> </li> <li class="related-post-item"> <a href="/blog/2022/Efficiency_Comparison/">Comparing the Efficiency of ChebConv, GCN, and SGC</a> <p>A comprehensive efficiency comparison of three popular spectral graph neural network models</p> </li> <li class="related-post-item"> <a href="/blog/2024/GraphKAN_experiment/">Experiments on using Kolmogorov-Arnold Networks (KAN) on Graph Learning (Github link)</a> <p>Exploring the application of KAN architecture to graph neural networks</p> </li> </ul> </section> <div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"jordan7186/jordan7186.github.io","data-repo-id":"","data-category":"Comments","data-category-id":"","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </article> </div> <footer class="fixed-bottom"> <div class="container mt-0 text-center"> Â© 2025 Yong-Min Shin</div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?07b8786bab9b4abe90d10e61f7d12ff7" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>