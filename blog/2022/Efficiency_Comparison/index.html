<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Comparing the Efficiency of ChebConv, GCN, and SGC | Yong-Min Shin</title> <meta name="author" content="Yong-Min Shin"> <meta name="description" content="A comprehensive efficiency comparison of three popular spectral graph neural network models"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="preconnect" href="https://fonts.googleapis.com"> <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin> <link href="https://fonts.googleapis.com/css2?family=Atkinson+Hyperlegible:ital,wght@0,400;0,700;1,400;1,700&amp;family=JetBrains+Mono:wght@400;500;600&amp;display=swap" rel="stylesheet"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A4%94&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://jordan7186.github.io/blog/2022/Efficiency_Comparison/"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand" href="/"><span class="brand-name">Yong-Min Shin</span></a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/presentations/">Presentations</a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container-fluid px-md-5 mt-5"> <article class="post-article"> <header class="post-header"> <h1 class="post-title">Comparing the Efficiency of ChebConv, GCN, and SGC</h1> <p class="post-subtitle">A comprehensive efficiency comparison of three popular spectral graph neural network models</p> <div class="post-meta-line"> <time datetime="2022-11-11T00:00:00+00:00">November 11, 2022</time> <span class="meta-separator">¬∑</span> <span class="read-time">12 min read</span> </div> </header> <div class="series-nav"> <div class="series-nav-header"> <span class="series-label">üìö Part 2 of 2</span> <span class="series-title">Graph Neural Networks</span> </div> <div class="series-nav-links"> <a href="/blog/2022/Revisiting_SSL/" class="series-nav-link series-nav-prev"> <span class="series-nav-direction">‚Üê Previous</span> <span class="series-nav-part-title">Revisiting Classical SSL Methods</span> </a> <div class="series-nav-link series-nav-placeholder"></div> </div> </div> <div class="post-content prose"> <h1 id="introduction">Introduction</h1> <p>In the current deep learning era that we live in, we now have various models that are widely used not only in research but also real-world problem solving in other data types such as image and natural languages: ResNet, YOLO, BERT, GPT, to name a few. The equivalent model in the field of graph data is perhaps the GCN model (Graph Convolutional Networks) <span class="citation" data-preview="Kipf, T. N., &amp; Welling, M. (2017). Semi-Supervised Classification with Graph Convolutional Networks. ICLR.">(Kipf &amp; Welling, 2017)</span>.</p> <p>GCN continues the effort made in ChebConv <span class="citation" data-preview="Defferrard, M., Bresson, X., &amp; Vandergheynst, P. (2016). Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering. NeurIPS.">(Defferrard et al., 2016)</span> in trying to build an efficient yet effective graph learning model, based on the framework of graph signal processing. Although not as commonly used but still well-known, SGC (Simple Graph Convolution) <span class="citation" data-preview="Wu, F., Souza, A., Zhang, T., Fifty, C., Yu, T., &amp; Weinberger, K. (2019). Simplifying Graph Convolutional Networks. ICML.">(Wu et al., 2019)</span> proposes an even simpler model on top of GCN, resulting in the most straightforward model to interpret out of the three models. In this post, we will attempt to comprehensively compare the three models in terms of efficiency.</p> <h1 id="architecture-of-three-models">Architecture of three models</h1> <h2 id="chebconv">ChebConv</h2> <p>As mentioned, GCN starts from the ChebConv model. First, let us define several concepts and notations:</p> <ul> <li>A graph \(G\) with the set of nodes \(V\) and edges \(E \subseteq V \times V\) is described as \(G = (V, E)\).</li> <li>Let us assume the graph is unweighted and undirected.</li> <li>The graph structure can be represented as a symmetric square matrix \(A \in \{0,1\}^{V \times V}\), where \(A_{ij} = 1\) iff \((i,j) \in E\).</li> <li>The degree matrix \(D\) is a diagonal matrix, where \(D_{ii}\) is the degree of node \(i\), and zero elsewhere.</li> <li>The (normalized) Laplacian matrix of \(G\) is denoted as \(L = I - D^{-1/2} A D^{-1/2}\). In GCN, we use a normalized version of the Laplacian: \(\tilde{L} = \dfrac{2}{\lambda_{\max}}L - I\), where the eigenvalues of \(\tilde{L}\) are normalized to the range of \([-1, 1]\).</li> </ul> <p>Then, the ChebConv layer can be described as:</p> \[\text{ChebConv} = \sum_{k=0}^{K-1}\theta_k T_k(\tilde{L})\] <p>, where \(T_k\) is the \(k\)-th basis function of the Chebyshev expansion and \(\theta_k\)‚Äôs are learnable parameters.</p> <h2 id="formulation-of-gcn">Formulation of GCN</h2> <p>Building upon Chebconv, GCN simplifies the convolution by only using the first two terms:</p> \[\theta_0 T_0(\tilde{L}) + \theta_1 T_1(\tilde{L}) = \theta_0 I + \theta_1 \tilde{L}\] <p>, and instead compensate by stacking multiple layers. Further approximating \(\lambda_{\max} = 2\), we can modify to:</p> \[\theta_0 I + \theta_1 \tilde{L} = \theta_0 I + \theta_1 (L - I) = \theta_0 I - \theta_1 D^{-1/2}AD^{-1/2}.\] <p>Furthermore, we can set $\theta_0 = -\theta_1 = \theta$ to reduce the number of parameters:</p> \[\theta(I + D^{-1/2}AD^{-1/2})\] <p>Finally, using the renormalization trick to keep the range of eigenvalues within \([0,1]\), we are left the following convolution layer:</p> \[\tilde{D}^{-1/2}\tilde{A}\tilde{D}^{-1/2}\theta\] <p>, where \(\tilde{A} = A + I\), and \(\tilde{D}\) follows a similar definition from \(D\). For a general \(C\)-dimensional graph signal \(X \in \mathbb{R}^{\lvert V \rvert \times C}\), we get the final convolution for GCN:</p> \[\text{GCNConv} = \tilde{D}^{-1/2}\tilde{A}\tilde{D}^{-1/2}X\Theta\] <p>, where \(\Theta \in \mathbb{R}^{C \times d}\) is a learnable matrix which maps the \(C\)-dimensional convoluted signal to a \(d\)-dimensional representation. Denoting \(\sigma\) as some non-linear activation function (e.g., ReLU) and \(\bar{A} = \tilde{D}^{-1/2}A\tilde{D}^{-1/2}\) for simplicity, stacking \(L\) layers of the convolution results in the following model architecture (assuming we are solving node classification):</p> \[\text{GCN}(A, X) = \text{softmax}(\bar{A}\sigma(\bar{A}\sigma(\cdots\sigma(\bar{A}\sigma(\bar{A}X\Theta_{1})\Theta_2)\cdots)\Theta_{L-1})\Theta_{L}).\] <p>Here, the set of parameter matrices \(\mathbf{\Theta} = \{\Theta_1, \cdots ,\Theta_L\}\) are learned using gradient descent.</p> <h2 id="formulation-of-sgc">Formulation of SGC</h2> <p>As the name suggests, SGC aims to simplify GCNs even further. Starting from the original equation of GCN above, they remove most of the non-linearity in the GCN model:</p> \[\text{softmax}(\bar{A}\bar{A}\cdots\bar{A}\bar{A}X\Theta_{1}\Theta_2\cdots\Theta_{L-1}\Theta_{L}) = \text{softmax}(\bar{A}^L X\Theta_{1}\Theta_2\cdots\Theta_{L-1}\Theta_{L}).\] <p>Removing the non-linearities further enables to reduce all parameter matrices to just one, as we can just model compositions of multiple linear transformations as a single linear transformation:</p> \[\text{softmax}(\bar{A}^L X\Theta_{1}\Theta_2\cdots\Theta_{L-1}\Theta_{L}) = \text{softmax}(\bar{A}^L X\Theta).\] <h1 id="analysis-of-efficiency">Analysis of efficiency</h1> <p>Here, we aim to analyze the efficiency of the three models. Specifically, the model configuration is made as follows to ensure the receptive field remains the same:</p> <ul> <li> <strong>ChebConv</strong>: 1 layer model with \(K=2,3,4,5\).</li> </ul> \[f^{\text{Cheb}}_{K}(A, X) = \text{softmax}(\sum_{i=0}^{K-1}\theta_iT_{i}(L)X)\] <ul> <li> <strong>GCN</strong>: $L$ layer model with \(L=1,2,3,4\). The equation below shows the case when \(L=2\).</li> </ul> \[f^{\text{GCN}}_{L=2}(A, X) = \text{softmax}(\bar{A}\sigma(\bar{A}X\Theta_{1})\Theta_2).\] <ul> <li> <strong>SGC</strong>: 1 layer model with \(L=1,2,3,4\). The equation below shows the case when \(L=2\).</li> </ul> \[f^{\text{SGC}}_{L=2}(A, X) = \text{softmax}(\bar{A}^2 X \Theta)\] <p>For the implementation, we will use <strong>pytorch</strong> with <strong>pytorch geometric</strong> as the main library, and the code for the model architecture is as follows:</p> <ul> <li> <p>ChebConv</p> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>  <span class="kn">import</span> <span class="n">torch</span>
  <span class="kn">from</span> <span class="n">torch_geometric.nn</span> <span class="kn">import</span> <span class="n">ChebConv</span>
    
  <span class="k">class</span> <span class="nc">GNN_Cheb</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
      <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">K</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">7</span><span class="p">):</span>
          <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
    
          <span class="n">self</span><span class="p">.</span><span class="n">convs</span> <span class="o">=</span> <span class="nc">ChebConv</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">K</span><span class="o">=</span><span class="n">K</span><span class="p">)</span>
    
      <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
          <span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="n">data</span><span class="p">.</span><span class="n">edge_index</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">convs</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="o">=</span><span class="n">edge_index</span><span class="p">)</span>
          <span class="k">return</span> <span class="n">x</span>
</code></pre></div> </div> </li> <li> <p>GCN</p> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>  <span class="kn">import</span> <span class="n">torch</span>
  <span class="kn">from</span> <span class="n">torch_geometric.nn</span> <span class="kn">import</span> <span class="n">GCNConv</span>
    
  <span class="k">class</span> <span class="nc">GNN_GCN</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
      <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
          <span class="n">self</span><span class="p">,</span>
          <span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
          <span class="n">hidden_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="c1"># default: 128
</span>          <span class="n">num_layers</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
          <span class="n">out_channels</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">7</span><span class="p">,</span>
          <span class="n">cached</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span><span class="p">,</span>
      <span class="p">):</span>
          <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
          <span class="n">self</span><span class="p">.</span><span class="n">convs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">()</span>
          <span class="k">if</span> <span class="n">num_layers</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
              <span class="n">self</span><span class="p">.</span><span class="n">convs</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span>
                  <span class="nc">GCNConv</span><span class="p">(</span>
                      <span class="n">in_channels</span><span class="o">=</span><span class="n">in_channels</span><span class="p">,</span>
                      <span class="n">out_channels</span><span class="o">=</span><span class="n">hidden_dim</span><span class="p">,</span>
                      <span class="n">add_self_loops</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                      <span class="n">cached</span><span class="o">=</span><span class="n">cached</span><span class="p">,</span>
                  <span class="p">)</span>
              <span class="p">)</span>
              <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_layers</span> <span class="o">-</span> <span class="mi">2</span><span class="p">):</span>
                  <span class="n">self</span><span class="p">.</span><span class="n">convs</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span>
                      <span class="nc">GCNConv</span><span class="p">(</span>
                          <span class="n">in_channels</span><span class="o">=</span><span class="n">hidden_dim</span><span class="p">,</span>
                          <span class="n">out_channels</span><span class="o">=</span><span class="n">hidden_dim</span><span class="p">,</span>
                          <span class="n">add_self_loops</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                          <span class="n">cached</span><span class="o">=</span><span class="n">cached</span><span class="p">,</span>
                      <span class="p">)</span>
                  <span class="p">)</span>
              <span class="n">self</span><span class="p">.</span><span class="n">convs</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span>
                  <span class="nc">GCNConv</span><span class="p">(</span>
                      <span class="n">in_channels</span><span class="o">=</span><span class="n">hidden_dim</span><span class="p">,</span>
                      <span class="n">out_channels</span><span class="o">=</span><span class="n">out_channels</span><span class="p">,</span>
                      <span class="n">add_self_loops</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                      <span class="n">cached</span><span class="o">=</span><span class="n">cached</span><span class="p">,</span>
                  <span class="p">)</span>
              <span class="p">)</span>
          <span class="k">else</span><span class="p">:</span>  <span class="c1"># num_layers == 1
</span>              <span class="n">self</span><span class="p">.</span><span class="n">convs</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span>
                  <span class="nc">GCNConv</span><span class="p">(</span>
                      <span class="n">in_channels</span><span class="o">=</span><span class="n">in_channels</span><span class="p">,</span>
                      <span class="n">out_channels</span><span class="o">=</span><span class="n">out_channels</span><span class="p">,</span>
                      <span class="n">add_self_loops</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                      <span class="n">cached</span><span class="o">=</span><span class="n">cached</span><span class="p">,</span>
                  <span class="p">)</span>
              <span class="p">)</span>
    
      <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
          <span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="n">data</span><span class="p">.</span><span class="n">edge_index</span>
          <span class="k">for</span> <span class="n">conv</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">convs</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
              <span class="n">x</span> <span class="o">=</span> <span class="nf">conv</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="o">=</span><span class="n">edge_index</span><span class="p">)</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">convs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">)</span>
          <span class="k">return</span> <span class="n">x</span>
</code></pre></div> </div> </li> <li> <p>SGC</p> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>  <span class="kn">import</span> <span class="n">torch</span>
  <span class="kn">from</span> <span class="n">torch_geometric.nn</span> <span class="kn">import</span> <span class="n">SGConv</span>
    
  <span class="k">class</span> <span class="nc">GNN_SGC</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
      <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
          <span class="n">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">K</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">7</span><span class="p">,</span> <span class="n">cache</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span>
      <span class="p">):</span>
          <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
          <span class="n">self</span><span class="p">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="nc">SGConv</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">K</span><span class="o">=</span><span class="n">K</span><span class="p">,</span> <span class="n">cached</span><span class="o">=</span><span class="n">cache</span><span class="p">)</span>
    
      <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
          <span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="n">data</span><span class="p">.</span><span class="n">edge_index</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">)</span>
          <span class="k">return</span> <span class="n">x</span>
</code></pre></div> </div> </li> </ul> <p>As for the measurement of <strong>efficiency</strong>, we take the message from <span class="citation" data-preview="Dehghani, M., et al. (2022). The Efficiency Misnomer. ICLR.">(Dehghani et al., 2022)</span> to heart. The authors suggest to draw plots using as multiple cost indicators, rather than highlighting only one. In this experiment, we consider the following cost indicators:</p> <ul> <li>FLOPs</li> <li>MACs</li> <li>Speed, measured by sec/graph (Note that in our experiments, we discard the use of mini-batch node sampling from GraphSAGE (Hamilton et al., 2017) and always use full-batch. Therefore the ‚Äòsec/example‚Äô from (Dehghani et al., 2022) naturally becomes sec/graph).</li> </ul> <p>This measures of cost indicator is quite different from the analysis made in the original SGC paper, as the main cost indicator is relative training time and wall-clock time. Also, ChebConv was not analyzed, and we expect it may bring potential discussion points.</p> <h2 id="experiment">Experiment</h2> <p>In the experiments, we will use the <em>deepspeed</em> library as the main profile tool for our analysis. The profiler in the experiments are built as:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="n">deepspeed.profiling.flops_profiler.profiler</span> <span class="kn">import</span> <span class="n">FlopsProfiler</span>


<span class="k">class</span> <span class="nc">Custom_Profiler</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">model</span><span class="p">):</span>
        <span class="k">assert</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">get_model_profile</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">model</span>
        <span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
        <span class="n">prof</span> <span class="o">=</span> <span class="nc">FlopsProfiler</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

        <span class="n">prof</span><span class="p">.</span><span class="nf">start_profile</span><span class="p">()</span>
        <span class="n">_</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="n">flops</span> <span class="o">=</span> <span class="n">prof</span><span class="p">.</span><span class="nf">get_total_flops</span><span class="p">()</span>
        <span class="n">macs</span> <span class="o">=</span> <span class="n">prof</span><span class="p">.</span><span class="nf">get_total_macs</span><span class="p">()</span>
        <span class="n">latency</span> <span class="o">=</span> <span class="n">prof</span><span class="p">.</span><span class="nf">get_total_duration</span><span class="p">()</span>
        <span class="n">prof</span><span class="p">.</span><span class="nf">end_profile</span><span class="p">()</span>
        <span class="n">prof</span><span class="p">.</span><span class="nf">reset_profile</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">flops</span><span class="p">,</span> <span class="n">macs</span><span class="p">,</span> <span class="n">latency</span></code></pre></figure> <p>Now, lets plot FLOPs, MACs and latency of ChebConv, GCN, SGC for different receptive fields:</p> <figure class="figure-numbered"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog4_1-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog4_1-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog4_1-1400.webp"></source> <img src="/assets/img/blog4_1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <figcaption>Plot of FLOPs vs. receptive field.</figcaption> </figure> <figure class="figure-numbered"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog4_2-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog4_2-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog4_2-1400.webp"></source> <img src="/assets/img/blog4_2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <figcaption>Plot of MACs vs. receptive field.</figcaption> </figure> <figure class="figure-numbered"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog4_3-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog4_3-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog4_3-1400.webp"></source> <img src="/assets/img/blog4_3.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <figcaption>Plot of Latency (sec/graph) vs. receptive field.</figcaption> </figure> <ul> <li>For all three measures of cost indicators, ChebConv performs the worst out of all model architectures considered.</li> <li>For FLOPs and MACs, the difference between GCN and SGC are quite marginal, even as we stack the number of layers.</li> <li>For latency, thing are not as simple. When we do not use caching, SGC actually is quite slow compared to GCN. However, SGC becomes <em>very</em> fast with caching enabled, even beating GCNs with caching. Therefore it is strongly advised that we enable caching to True for SGC.</li> </ul> <h1 id="conclusion">Conclusion</h1> <p>Here, we have compared three important graph convolutional methods, ChebConv, GCN, and SGC. Experiments observing FLOPs, MACs and latency indicate that GCN and SGC are significantly efficient compared to ChebConv, and also the use of caching is critical in latency for SGC models.</p> </div> <section class="post-references"> <h2>References</h2> <ol class="reference-list"> <li> Shuman, D. I., Narang, S. K., Frossard, P., Ortega, A., &amp; Vandergheynst, P. (2013). <a href="https://arxiv.org/abs/1211.0053" rel="external nofollow noopener" target="_blank">The Emerging Field of Signal Processing on Graphs</a>. IEEE Signal Processing Magazine. </li> <li> Defferrard, M., Bresson, X., &amp; Vandergheynst, P. (2016). <a href="https://arxiv.org/abs/1606.09375" rel="external nofollow noopener" target="_blank">Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering</a>. NeurIPS. </li> <li> Wu, F., Souza, A., Zhang, T., Fifty, C., Yu, T., &amp; Weinberger, K. (2019). <a href="https://arxiv.org/abs/1902.07153" rel="external nofollow noopener" target="_blank">Simplifying Graph Convolutional Networks</a>. ICML. </li> <li> Dehghani, M., et al. (2022). <a href="https://arxiv.org/abs/2110.12894" rel="external nofollow noopener" target="_blank">The Efficiency Misnomer</a>. ICLR. </li> <li> Hamilton, W., Ying, Z., &amp; Leskovec, J. (2017). <a href="https://arxiv.org/abs/1706.02216" rel="external nofollow noopener" target="_blank">Inductive Representation Learning on Large Graphs</a>. NeurIPS. </li> </ol> </section> <section class="related-posts"> <h2>Other Posts</h2> <ul class="related-posts-list"> <li class="related-post-item"> <a href="/blog/2022/ChebConv/">Understanding ChebConv with the NEIGHBORSMATCH problem</a> <p>Deep dive into Chebyshev spectral graph convolutions and analysis using the NEIGHBORSMATCH problem</p> </li> <li class="related-post-item"> <a href="/blog/2022/Cora_spectral/">Dissecting Cora in the Spectral Domain</a> <p>Graph signal processing and Fourier analysis on the Cora dataset</p> </li> <li class="related-post-item"> <a href="/blog/2022/Revisiting_SSL/">Revisiting Modern Benchmarks for Semi-suparvised Node Classification using Classical Methods</a> <p>Applying classical label propagation methods to modern graph benchmark datasets</p> </li> <li class="related-post-item"> <a href="/blog/2024/Fidelity_pruning/">On the feasibility of fidelity- for graph pruning</a> <p>Exploring whether GNN explanation methods can be repurposed for graph pruning</p> </li> <li class="related-post-item"> <a href="/blog/2024/GraphKAN_experiment/">Experiments on using Kolmogorov-Arnold Networks (KAN) on Graph Learning (Github link)</a> <p>Exploring the application of KAN architecture to graph neural networks</p> </li> </ul> </section> <div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"jordan7186/jordan7186.github.io","data-repo-id":"","data-category":"Comments","data-category-id":"","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </article> </div> <footer class="fixed-bottom"> <div class="container mt-0 text-center"> ¬© 2025 Yong-Min Shin</div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?07b8786bab9b4abe90d10e61f7d12ff7" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>