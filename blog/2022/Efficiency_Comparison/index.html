<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Comparing the Efficiency of ChebConv, GCN, and SGC | Yong-Min Shin</title> <meta name="author" content="Yong-Min Shin"> <meta name="description" content="Welcome to my homepage! "> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A4%94&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://jordan7186.github.io/blog/2022/Efficiency_Comparison/"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Yong-Min Shin</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Graph Learning Blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/presentations/">Presentations</a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Comparing the Efficiency of ChebConv, GCN, and SGC</h1> <p class="post-meta">November 11, 2022</p> <p class="post-tags"> <a href="/blog/2022"> <i class="fa-solid fa-calendar fa-sm"></i> 2022 </a>   ·   <a href="/blog/tag/graph-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Graph_learning</a>     ·   <a href="/blog/category/efficiency"> <i class="fa-solid fa-tag fa-sm"></i> Efficiency</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <h1 id="introduction">Introduction</h1> <p>In the current deep learning era that we live in, we now have various models that are widely used not only in research but also real-world problem solving in other data types such as image and natural languages: ResNet, YOLO, BERT, GPT, to name a few. The equivalent model in the field of graph data is perhaps the GCN model (Graph Convolutional Networks) (Kipf &amp; Welling, 2017).</p> <p>GCN continues the effort made in ChebConv (Defferrard et al., 2016) in trying to build an efficient yet effective graph learning model, based on the framework of graph signal processing. Although not as commonly used but still well-known, SGC (Simple Graph Convolution) (Wu et al., 2019) proposes an even simpler model on top of GCN, resulting in the most straightforward model to interpret out of the three models. In this post, we will attempt to comprehensively compare the three models in terms of efficiency.</p> <h1 id="architecture-of-three-models">Architecture of three models</h1> <h2 id="chebconv">ChebConv</h2> <p>As mentioned, GCN starts from the ChebConv model. First, let us define several concepts and notations:</p> <ul> <li>A graph \(G\) with the set of nodes \(V\) and edges \(E \subseteq V \times V\) is described as \(G = (V, E)\).</li> <li>Let us assume the graph is unweighted and undirected.</li> <li>The graph structure can be represented as a symmetric square matrix \(A \in \{0,1\}^{V \times V}\), where \(A_{ij} = 1\) iff \((i,j) \in E\).</li> <li>The degree matrix \(D\) is a diagonal matrix, where \(D_{ii}\) is the degree of node \(i\), and zero elsewhere.</li> <li>The (normalized) Laplacian matrix of \(G\) is denoted as \(L = I - D^{-1/2} A D^{-1/2}\). In GCN, we use a normalized version of the Laplacian: \(\tilde{L} = \dfrac{2}{\lambda_{\max}}L - I\), where the eigenvalues of \(\tilde{L}\) are normalized to the range of \([-1, 1]\).</li> </ul> <p>Then, the ChebConv layer can be described as:</p> \[\text{ChebConv} = \sum_{k=0}^{K-1}\theta_k T_k(\tilde{L})\] <p>, where \(T_k\) is the \(k\)-th basis function of the Chebyshev expansion and \(\theta_k\)’s are learnable parameters.</p> <h2 id="formulation-of-gcn">Formulation of GCN</h2> <p>Building upon Chebconv, GCN simplifies the convolution by only using the first two terms:</p> \[\theta_0 T_0(\tilde{L}) + \theta_1 T_1(\tilde{L}) = \theta_0 I + \theta_1 \tilde{L}\] <p>, and instead compensate by stacking multiple layers. Further approximating \(\lambda_{\max} = 2\), we can modify to:</p> \[\theta_0 I + \theta_1 \tilde{L} = \theta_0 I + \theta_1 (L - I) = \theta_0 I - \theta_1 D^{-1/2}AD^{-1/2}.\] <p>Furthermore, we can set $\theta_0 = -\theta_1 = \theta$ to reduce the number of parameters:</p> \[\theta(I + D^{-1/2}AD^{-1/2})\] <p>Finally, using the renormalization trick to keep the range of eigenvalues within \([0,1]\), we are left the following convolution layer:</p> \[\tilde{D}^{-1/2}\tilde{A}\tilde{D}^{-1/2}\theta\] <table> <tbody> <tr> <td>, where \(\tilde{A} = A + I\), and \(\tilde{D}\) follows a similar definition from \(D\). For a general \(C\)-dimensional graph signal $$X \in \mathbb{R}^{</td> <td>V</td> <td>\times C}$$, we get the final convolution for GCN:</td> </tr> </tbody> </table> \[\text{GCNConv} = \tilde{D}^{-1/2}\tilde{A}\tilde{D}^{-1/2}X\Theta\] <p>, where \(\Theta \in \mathbb{R}^{C \times d}\) is a learnable matrix which maps the \(C\)-dimensional convoluted signal to a \(d\)-dimensional representation. Denoting \(\sigma\) as some non-linear activation function (e.g., ReLU) and \(\bar{A} = \tilde{D}^{-1/2}A\tilde{D}^{-1/2}\) for simplicity, stacking \(L\) layers of the convolution results in the following model architecture (assuming we are solving node classification):</p> \[\text{GCN}(A, X) = \text{softmax}(\bar{A}\sigma(\bar{A}\sigma(\cdots\sigma(\bar{A}\sigma(\bar{A}X\Theta_{1})\Theta_2)\cdots)\Theta_{L-1})\Theta_{L}).\] <p>Here, the set of parameter matrices \(\mathbf{\Theta} = \{\Theta_1, \cdots ,\Theta_L\}\) are learned using gradient descent.</p> <h2 id="formulation-of-sgc">Formulation of SGC</h2> <p>As the name suggests, SGC aims to simplify GCNs even further. Starting from the original equation of GCN above, they remove most of the non-linearity in the GCN model:</p> \[\text{softmax}(\bar{A}\bar{A}\cdots\bar{A}\bar{A}X\Theta_{1}\Theta_2\cdots\Theta_{L-1}\Theta_{L}) = \text{softmax}(\bar{A}^L X\Theta_{1}\Theta_2\cdots\Theta_{L-1}\Theta_{L}).\] <p>Removing the non-linearities further enables to reduce all parameter matrices to just one, as we can just model compositions of multiple linear transformations as a single linear transformation:</p> \[\text{softmax}(\bar{A}^L X\Theta_{1}\Theta_2\cdots\Theta_{L-1}\Theta_{L}) = \text{softmax}(\bar{A}^L X\Theta).\] <h1 id="analysis-of-efficiency">Analysis of efficiency</h1> <p>Here, we aim to analyze the efficiency of the three models. Specifically, the model configuration is made as follows to ensure the receptive field remains the same:</p> <ul> <li> <strong>ChebConv</strong>: 1 layer model with \(K=2,3,4,5\).</li> </ul> \[f^{\text{Cheb}}_{K}(A, X) = \text{softmax}(\sum_{i=0}^{K-1}\theta_iT_{i}(L)X)\] <ul> <li> <strong>GCN</strong>: $L$ layer model with \(L=1,2,3,4\). The equation below shows the case when \(L=2\).</li> </ul> \[f^{\text{GCN}}_{L=2}(A, X) = \text{softmax}(\bar{A}\sigma(\bar{A}X\Theta_{1})\Theta_2).\] <ul> <li> <strong>SGC</strong>: 1 layer model with \(L=1,2,3,4\). The equation below shows the case when \(L=2\).</li> </ul> \[f^{\text{SGC}}_{L=2}(A, X) = \text{softmax}(\bar{A}^2 X \Theta)\] <p>For the implementation, we will use <strong>pytorch</strong> with <strong>pytorch geometric</strong> as the main library, and the code for the model architecture is as follows:</p> <ul> <li> <p>ChebConv</p> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>  <span class="kn">import</span> <span class="n">torch</span>
  <span class="kn">from</span> <span class="n">torch_geometric.nn</span> <span class="kn">import</span> <span class="n">ChebConv</span>
    
  <span class="k">class</span> <span class="nc">GNN_Cheb</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
      <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">K</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">7</span><span class="p">):</span>
          <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
    
          <span class="n">self</span><span class="p">.</span><span class="n">convs</span> <span class="o">=</span> <span class="nc">ChebConv</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">K</span><span class="o">=</span><span class="n">K</span><span class="p">)</span>
    
      <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
          <span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="n">data</span><span class="p">.</span><span class="n">edge_index</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">convs</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="o">=</span><span class="n">edge_index</span><span class="p">)</span>
          <span class="k">return</span> <span class="n">x</span>
</code></pre></div> </div> </li> <li> <p>GCN</p> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>  <span class="kn">import</span> <span class="n">torch</span>
  <span class="kn">from</span> <span class="n">torch_geometric.nn</span> <span class="kn">import</span> <span class="n">GCNConv</span>
    
  <span class="k">class</span> <span class="nc">GNN_GCN</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
      <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
          <span class="n">self</span><span class="p">,</span>
          <span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
          <span class="n">hidden_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="c1"># default: 128
</span>          <span class="n">num_layers</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
          <span class="n">out_channels</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">7</span><span class="p">,</span>
          <span class="n">cached</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span><span class="p">,</span>
      <span class="p">):</span>
          <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
          <span class="n">self</span><span class="p">.</span><span class="n">convs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">()</span>
          <span class="k">if</span> <span class="n">num_layers</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
              <span class="n">self</span><span class="p">.</span><span class="n">convs</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span>
                  <span class="nc">GCNConv</span><span class="p">(</span>
                      <span class="n">in_channels</span><span class="o">=</span><span class="n">in_channels</span><span class="p">,</span>
                      <span class="n">out_channels</span><span class="o">=</span><span class="n">hidden_dim</span><span class="p">,</span>
                      <span class="n">add_self_loops</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                      <span class="n">cached</span><span class="o">=</span><span class="n">cached</span><span class="p">,</span>
                  <span class="p">)</span>
              <span class="p">)</span>
              <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_layers</span> <span class="o">-</span> <span class="mi">2</span><span class="p">):</span>
                  <span class="n">self</span><span class="p">.</span><span class="n">convs</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span>
                      <span class="nc">GCNConv</span><span class="p">(</span>
                          <span class="n">in_channels</span><span class="o">=</span><span class="n">hidden_dim</span><span class="p">,</span>
                          <span class="n">out_channels</span><span class="o">=</span><span class="n">hidden_dim</span><span class="p">,</span>
                          <span class="n">add_self_loops</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                          <span class="n">cached</span><span class="o">=</span><span class="n">cached</span><span class="p">,</span>
                      <span class="p">)</span>
                  <span class="p">)</span>
              <span class="n">self</span><span class="p">.</span><span class="n">convs</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span>
                  <span class="nc">GCNConv</span><span class="p">(</span>
                      <span class="n">in_channels</span><span class="o">=</span><span class="n">hidden_dim</span><span class="p">,</span>
                      <span class="n">out_channels</span><span class="o">=</span><span class="n">out_channels</span><span class="p">,</span>
                      <span class="n">add_self_loops</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                      <span class="n">cached</span><span class="o">=</span><span class="n">cached</span><span class="p">,</span>
                  <span class="p">)</span>
              <span class="p">)</span>
          <span class="k">else</span><span class="p">:</span>  <span class="c1"># num_layers == 1
</span>              <span class="n">self</span><span class="p">.</span><span class="n">convs</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span>
                  <span class="nc">GCNConv</span><span class="p">(</span>
                      <span class="n">in_channels</span><span class="o">=</span><span class="n">in_channels</span><span class="p">,</span>
                      <span class="n">out_channels</span><span class="o">=</span><span class="n">out_channels</span><span class="p">,</span>
                      <span class="n">add_self_loops</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                      <span class="n">cached</span><span class="o">=</span><span class="n">cached</span><span class="p">,</span>
                  <span class="p">)</span>
              <span class="p">)</span>
    
      <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
          <span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="n">data</span><span class="p">.</span><span class="n">edge_index</span>
          <span class="k">for</span> <span class="n">conv</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">convs</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
              <span class="n">x</span> <span class="o">=</span> <span class="nf">conv</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="o">=</span><span class="n">edge_index</span><span class="p">)</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">convs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">)</span>
          <span class="k">return</span> <span class="n">x</span>
</code></pre></div> </div> </li> <li> <p>SGC</p> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>  <span class="kn">import</span> <span class="n">torch</span>
  <span class="kn">from</span> <span class="n">torch_geometric.nn</span> <span class="kn">import</span> <span class="n">SGConv</span>
    
  <span class="k">class</span> <span class="nc">GNN_SGC</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
      <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
          <span class="n">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">K</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">7</span><span class="p">,</span> <span class="n">cache</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span>
      <span class="p">):</span>
          <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
          <span class="n">self</span><span class="p">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="nc">SGConv</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">K</span><span class="o">=</span><span class="n">K</span><span class="p">,</span> <span class="n">cached</span><span class="o">=</span><span class="n">cache</span><span class="p">)</span>
    
      <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
          <span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="n">data</span><span class="p">.</span><span class="n">edge_index</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">)</span>
          <span class="k">return</span> <span class="n">x</span>
</code></pre></div> </div> </li> </ul> <p>As for the measurement of <strong>efficiency</strong>, we take the message from [4] to heart. The authors of [4] suggest to draw plots using as multiple cost indicators, rather than highlighting only one. In this experiment, we consider the following cost indicators:</p> <ul> <li>FLOPs</li> <li>MACs</li> <li>Speed, measured by sec/graph (Note that in our experiments, we discard the use of mini-batch node sampling from GraphSAGE (Hamilton et al., 2017) and always use full-batch. Therefore the ‘sec/example’ from (Dehghani et al., 2022) naturally becomes sec/graph).</li> </ul> <p>This measures of cost indicator is quite different from the analysis made in the original SGC paper, as the main cost indicator is relative training time and wall-clock time. Also, ChebConv was not analyzed, and we expect it may bring potential discussion points.</p> <h2 id="experiment">Experiment</h2> <p>In the experiments, we will use the <em>deepspeed</em> library as the main profile tool for our analysis. The profiler in the experiments are built as:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="n">deepspeed.profiling.flops_profiler.profiler</span> <span class="kn">import</span> <span class="n">FlopsProfiler</span>


<span class="k">class</span> <span class="nc">Custom_Profiler</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">model</span><span class="p">):</span>
        <span class="k">assert</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">get_model_profile</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">model</span>
        <span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
        <span class="n">prof</span> <span class="o">=</span> <span class="nc">FlopsProfiler</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

        <span class="n">prof</span><span class="p">.</span><span class="nf">start_profile</span><span class="p">()</span>
        <span class="n">_</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="n">flops</span> <span class="o">=</span> <span class="n">prof</span><span class="p">.</span><span class="nf">get_total_flops</span><span class="p">()</span>
        <span class="n">macs</span> <span class="o">=</span> <span class="n">prof</span><span class="p">.</span><span class="nf">get_total_macs</span><span class="p">()</span>
        <span class="n">latency</span> <span class="o">=</span> <span class="n">prof</span><span class="p">.</span><span class="nf">get_total_duration</span><span class="p">()</span>
        <span class="n">prof</span><span class="p">.</span><span class="nf">end_profile</span><span class="p">()</span>
        <span class="n">prof</span><span class="p">.</span><span class="nf">reset_profile</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">flops</span><span class="p">,</span> <span class="n">macs</span><span class="p">,</span> <span class="n">latency</span></code></pre></figure> <p>Now, lets plot FLOPs, MACs and latency of ChebConv, GCN, SGC for different receptive fields:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog4_1-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog4_1-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog4_1-1400.webp"></source> <img src="/assets/img/blog4_1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Plot of FLOPs vs. receptive field. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog4_2-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog4_2-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog4_2-1400.webp"></source> <img src="/assets/img/blog4_2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Plot of MACs vs. receptive field. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog4_3-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog4_3-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog4_3-1400.webp"></source> <img src="/assets/img/blog4_3.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Plot of Latency (sec/graph) vs. receptive field. </div> <ul> <li>For all three measures of cost indicators, ChebConv performs the worst out of all model architectures considered.</li> <li>For FLOPs and MACs, the difference between GCN and SGC are quite marginal, even as we stack the number of layers.</li> <li>For latency, thing are not as simple. When we do not use caching, SGC actually is quite slow compared to GCN. However, SGC becomes <em>very</em> fast with caching enabled, even beating GCNs with caching. Therefore it is strongly advised that we enable caching to True for SGC.</li> </ul> <h1 id="conclusion">Conclusion</h1> <p>Here, we have compared three important graph convolutional methods, ChebConv, GCN, and SGC. Experiments observing FLOPs, MACs and latency indicate that GCN and SGC are significantly efficient compared to ChebConv, and also the use of caching is critical in latency for SGC models.</p> </div> </article> <h2>References</h2> <div class="publications"> <h2 class="bibliography">2022</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ICLR</abbr></div> <div id="Dehghani2022efficiency" class="col-sm-8"> <div class="title">The Efficiency Misnomer</div> <div class="author"> Mostafa Dehghani, Yi Tay, Anurag Arnab, Lucas Beyer, and Ashish Vaswani</div> <div class="periodical"> <em>In ICLR</em>, Apr 2022 </div> <div class="periodical"> </div> <div class="links"> <a href="https://openreview.net/forum?id=iulEMLYh1uR" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> </div> </div> </li></ol> <h2 class="bibliography">2019</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ICML</abbr></div> <div id="Wu2019sgc" class="col-sm-8"> <div class="title">Simplifying Graph Convolutional Networks</div> <div class="author"> Felix Wu, Amauri H. Souza Jr., Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Q. Weinberger</div> <div class="periodical"> <em>In ICML</em>, Jun 2019 </div> <div class="periodical"> </div> <div class="links"> <a href="http://proceedings.mlr.press/v97/wu19e.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> </div> </div> </li></ol> <h2 class="bibliography">2017</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">NeurIPS</abbr></div> <div id="Hamilton2017graphsage" class="col-sm-8"> <div class="title">Inductive Representation Learning on Large Graphs</div> <div class="author"> William L. Hamilton, Zhitao Ying, and Jure Leskovec</div> <div class="periodical"> <em>In NeurIPS</em>, Dec 2017 </div> <div class="periodical"> </div> <div class="links"> <a href="https://proceedings.neurips.cc/paper/2017/hash/5dd9db5e033da9c6fb5ba83c7a7ebea9-Abstract.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> </div> </div> </li></ol> <h2 class="bibliography">2016</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">NeurIPS</abbr></div> <div id="Defferrard2016chebconv" class="col-sm-8"> <div class="title">Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering</div> <div class="author"> Michaël Defferrard, Xavier Bresson, and Pierre Vandergheynst</div> <div class="periodical"> <em>In NeurIPS</em>, Dec 2016 </div> <div class="periodical"> </div> <div class="links"> <a href="https://proceedings.neurips.cc/paper/2016/hash/04df4d434d481c5bb723be1b6df1ee65-Abstract.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> </div> </div> </li></ol> <h2 class="bibliography">2013</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="Shuman2013graphspectral" class="col-sm-8"> <div class="title">The Emerging Field of Signal Processing on Graphs: Extending High-Dimensional Data Analysis to Networks and Other Irregular Domains</div> <div class="author"> David I. Shuman, Sunil K. Narang, Pascal Frossard, Antonio Ortega, and Pierre Vandergheynst</div> <div class="periodical"> <em>IEEE Signal Process. Mag.</em>, Dec 2013 </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.1109/MSP.2012.2235192" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> </div> </div> </li></ol> </div> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/ChebConv/">Understanding ChebConv with the NEIGHBORSMATCH problem</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/Cora_spectral/">Dissecting Cora in the Spectral Domain</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/Revisiting_SSL/">Revisiting Modern Benchmarks for Semi-suparvised Node Classification using Classical Methods</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/Fidelity_pruning/">On the feasibility of fidelity- for graph pruning</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/GraphKAN_experiment/">Experiments on using Kolmogorov-Arnold Networks (KAN) on Graph Learning (Github link)</a> </li> <div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"jordan7186/jordan7186.github.io","data-repo-id":"","data-category":"Comments","data-category-id":"","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Yong-Min Shin. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?07b8786bab9b4abe90d10e61f7d12ff7" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>