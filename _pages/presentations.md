---
layout: page
permalink: /presentations/
title: Presentations
description: These are the selected presentation slides that I have used in internal reading groups, workshops, and conferences.
nav: true
nav_order: 3
---

## External presentations

#### On interpretation of GATs based on attention
This presentation was given before my short visit to UNSW Sydney, Australia, and it was a great opportunity to share my research (currently under review).

<div class="embed-container">
  <iframe src="https://drive.google.com/file/d/1c4HwetaWzszEaFwzqVkOA5hyDCty3Mlh/preview" width="640" height="360"></iframe>
</div>

#### Towards MLPs as effective graph learning
This presentation was given at the A3 workshop on 2023.02.21, and I was awarded the best presentation award.

<div class="embed-container">
  <iframe src="https://drive.google.com/file/d/1didj1MczkQi6FDmSpC3LQkDAIynsS1Bz/preview" width="640" height="360"></iframe>
</div>

#### PAGE: Prototype-Based Model-Level Explanations for Graph Neural Networks
This presentation was given in the IITP project meeting on 2022.05.20. The presentation introduces the PAGE framework, which at the time the expanded version of the paper was under review.

<div class="embed-container">
  <iframe src="https://drive.google.com/file/d/1SA2Dk9MeN1B9YBbszRkkbQTG0sYPT3og/preview" width="640" height="360"></iframe>
</div>

Also, this is the poster that I presented for the short version, accepted at AAAI 2022.

<div class="embed-container">
  <iframe src="https://drive.google.com/file/d/1G5zppNVts4XQ8LfuPyp-KqbgtlWlY7L1/preview" width="360" height="480"></iframe>
</div>

#### Edgeless-GNN: Unsupervised Inductive Edgeless Network Embedding
This presentation was given during the 28th Samsung HumanTech award, which eventually led to the award of the bronze prize.

<div class="embed-container">
  <iframe src="https://drive.google.com/file/d/1amqs6noFUlW0eYo8EuipCMdKMRHWmuki/preview" width="640" height="360"></iframe>
</div>


## Internal reading groups
Selected slides from the internal reading groups that I have presented. Preparing for these presentations has been a great way to learn about new topics and to keep up with the latest research with a variety of topics.

#### Graph learning and Graph neural networks

- [Introduction to graph mining and graph neural networks](https://drive.google.com/file/d/1ZXaw0l1Pmn6XJPMpdlpBMSurH7I8wQ5O/view?usp=sharing): My introductory-level presentation on the general field of graph learning and graph neural networks.

<div class="embed-container">
  <iframe src="https://drive.google.com/file/d/1ZXaw0l1Pmn6XJPMpdlpBMSurH7I8wQ5O/preview" width="640" height="360"></iframe>
</div>

- [Industry use cases of GNN-based recommender systems](https://drive.google.com/file/d/1zNM3VzsB01QBpwnHtiQp09E_YLr35CxU/view?usp=sharing): A survey of the industry use cases of graph neural networks (as recommender systems).

<div class="embed-container">
  <iframe src="https://drive.google.com/file/d/1zNM3VzsB01QBpwnHtiQp09E_YLr35CxU/preview" width="640" height="360"></iframe>
</div>

- [Accelerative GNNs with MLPs](https://drive.google.com/file/d/1Si991-iSc5O_wxqJwqITW5xDy0hquchm/view?usp=sharing): This was a presentation made when I was focused on developing different approaches to perform more efficient graph learning.

<div class="embed-container">
  <iframe src="https://drive.google.com/file/d/1Si991-iSc5O_wxqJwqITW5xDy0hquchm/preview" width="640" height="360"></iframe>
</div>

- [On the representational power of graph neural networks](https://drive.google.com/file/d/1TMm_ZlODbL7qkSqkxQ-z8XoqdQJT6-hG/view?usp=sharing): This was a presentation made during when I was trying to understand the research on the representational power of graph neural networks.

<div class="embed-container">
  <iframe src="https://drive.google.com/file/d/1TMm_ZlODbL7qkSqkxQ-z8XoqdQJT6-hG/preview" width="640" height="360"></iframe>
</div>

- A review of the paper: [Message passing all the way up](https://drive.google.com/file/d/14-J2Z7XGyT0CPbUnC7DJ5aZblVFAlrvB/view?usp=sharing) by Petar Veličković. I had such fun reading and expanding on this position paper. 

<div class="embed-container">
  <iframe src="https://drive.google.com/file/d/14-J2Z7XGyT0CPbUnC7DJ5aZblVFAlrvB/preview" width="640" height="360"></iframe>
</div>

- [On simple graph neural networks](https://drive.google.com/file/d/1viKd_dEFmJADzLYfKMmcjOgjS6KVHBjk/view?usp=sharing): This was an early presentation made when I was interested in developing simple graph learning frameworks.

<div class="embed-container">
  <iframe src="https://drive.google.com/file/d/1viKd_dEFmJADzLYfKMmcjOgjS6KVHBjk/preview" width="640" height="360"></iframe>
</div>

- [Homophily assumption and graph neural networks](https://drive.google.com/file/d/1nvHrDcrHwWeTEwZWNCh9uaYPSnXXhbCV/view?usp=sharing)

<div class="embed-container">
  <iframe src="https://drive.google.com/file/d/1nvHrDcrHwWeTEwZWNCh9uaYPSnXXhbCV/preview" width="640" height="360"></iframe>
</div>


#### Explainable AI and Interpretable machine learning

- [How much can we analyze attention?](https://drive.google.com/file/d/14Ypz0zj-O6JevY9mBxuQ3NuHzRno8ZM0/view?usp=sharing) and [Towards ``faithful" attention](https://drive.google.com/file/d/1mqdfEvP0KARprNGWITk7otMG6em4gEkK/view?usp=sharing) were presentations made when I was interested in exploring the interpretability of attention mechanisms in deep learning models. The effort eventually led to the research on interpreting GAT models with attention, which is currently under review.

<div class="embed-container">
  <iframe src="https://drive.google.com/file/d/14Ypz0zj-O6JevY9mBxuQ3NuHzRno8ZM0/preview" width="640" height="360"></iframe>
</div>

<div class="embed-container">
  <iframe src="https://drive.google.com/file/d/1mqdfEvP0KARprNGWITk7otMG6em4gEkK/preview" width="640" height="360"></iframe>
</div>

#### Adversarial attacks & defenses

- [Adversarial attacks in graphs and its defense](https://drive.google.com/file/d/1Xh2AmTRV17gXYQMQ09fapBUVwFljYJO7/view?usp=sharing): This was a presentation made when I was studying the literature on adversarial attacks and defenses in graph neural networks.

<div class="embed-container">
  <iframe src="https://drive.google.com/file/d/1Xh2AmTRV17gXYQMQ09fapBUVwFljYJO7/preview" width="640" height="360"></iframe>
</div>

#### Causal learning

This was a two-part presentation where I introduced the basics of causal learning, which I got a LOT of help from ["Introduction to Causal Inference" by Brady Neal](https://www.bradyneal.com/causal-inference-course). Thank you!!

<div class="embed-container">
  <iframe src="https://drive.google.com/file/d/1CUNenpkm3Jxxw0RPjqHTtCXu1phce08W/preview" width="640" height="360"></iframe>
</div>

<div class="embed-container">
  <iframe src="https://drive.google.com/file/d/1OCHw7_Mbm3qUFPxz9fIdxTI8KILV3WAv/preview" width="640" height="360"></iframe>
</div>

#### Self-supervised learning

- [Introduction to SimCLR](https://drive.google.com/file/d/1aafQ5f6egM1TGsZXehRzzp1Z6coHRhqY/view?usp=sharing): This was a presentation made when I was interested in understanding the SimCLR framework.

<div class="embed-container">
  <iframe src="https://drive.google.com/file/d/1aafQ5f6egM1TGsZXehRzzp1Z6coHRhqY/preview" width="640" height="360"></iframe>
</div>


#### Knowledge distillation

- [Towards understanding knowledge distillation](https://drive.google.com/file/d/1Nf40Y0ZuSJVzOlObBczgkJ0ksqeHzvg-/view?usp=sharing): This was a presentation made when I was interested in understanding knowledge distillation, which I eventually applied to my research on efficient graph learning.

<div class="embed-container">
  <iframe src="https://drive.google.com/file/d/1Nf40Y0ZuSJVzOlObBczgkJ0ksqeHzvg-/preview" width="640" height="360"></iframe>
</div>

#### Physics-informed machine learning (PIML)

- A review of the paper: [Vector Neurons: A General Framework for SO(3)-Equivariant Networks](https://drive.google.com/file/d/1BeaBhggwPEDEFIDJLQqg8mz_CN9tQe-p/view?usp=sharing) by Dent et al. This was a presentation made when I was interested in understanding the physics-informed machine learning landscape.

<div class="embed-container">
  <iframe src="https://drive.google.com/file/d/1BeaBhggwPEDEFIDJLQqg8mz_CN9tQe-p/preview" width="640" height="360"></iframe>
</div>


#### Unsupervised disentanglement

- A review of the paper: [Challenging common assumptions in the unsupervised learning of disentangled representations](https://drive.google.com/file/d/1HS-CQ_xUj0D8hbzmQORz0K7_B8ptG6gj/view?usp=sharing) by Locatello et al.

<div class="embed-container">
  <iframe src="https://drive.google.com/file/d/1HS-CQ_xUj0D8hbzmQORz0K7_B8ptG6gj/preview" width="640" height="360"></iframe>
</div>
