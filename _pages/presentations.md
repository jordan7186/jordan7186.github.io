---
layout: page
permalink: /presentations/
title: Presentations
description: These are the selected presentation slides that I have used in internal reading groups, workshops, and conferences.
nav: true
nav_order: 3
---

## External presentations

### Seminar series @ GIST

This is a series of seminars that I gave at Gwangju Institute of Science and Technology (GIST) as a guest speaker in Prof. Honk Kook Kim's group from March ~ May 2025. The seminar series covers the basics of graph neural networks, several fundamental concepts, and applications.

**Session 1:** [Introduction to graph mining and graph neural networks](/assets/pdf/[GIST]01_Introduction_to_graph_mining_and_graph_neural_networks.pdf)

**Session 2:** [On the representational power of graph neural networks](/assets/pdf/[GIST]02_On_the_representational_power_of_graph_neural_networks.pdf)

**Session 3:** [A graph signal processing viewpoint of graph neural networks](/assets/pdf/[GIST]03_A_graph_signal_processing_viewpoint_of_graph_neural_networks.pdf)

**Session 4:** [From label propagation to graph neural networks](/assets/pdf/[GIST]04_From_label_propagation_to_graph_neural_networks.pdf)

**Session 5:** [On the problem of oversmoothing and oversquashing](/assets/pdf/[GIST]05_On_the_problem_of_oversmoothing_and_oversquashing.pdf)

**Session 6:** [Towards efficient graph learning](/assets/pdf/[GIST]06_Towards_efficient_graph_learning.pdf)

**Session 7:** Explainable graph neural networks (TBA)

### Seminar @ Ehwa Womans University

This is a seminar that I gave at Ehwa Womans University as a guest speaker in April 2025.

**Slides**: [A practical introduction to (explainable) graph learning](/assets/pdf/Ewha_Uni_lecture.pdf)

### Faithful and Accurate Self-Attention Attribution for Message Passing Neural Networks via the Computation Tree Viewpoint

This is the poster for my paper "Faithful and Accurate Self-Attention Attribution for Message Passing Neural Networks via the Computation Tree Viewpoint" that was accepted at AAAI 2025. The arxiv version of the paper can be found [here](https://arxiv.org/abs/2406.04612).

**Poster:** [Faithful and Accurate Self-Attention Attribution for Message Passing Neural Networks via the Computation Tree Viewpoint](/assets/pdf/Poster_final_GAtt.pdf)

### PAGE: Prototype-Based Model-Level Explanations for Graph Neural Networks

This presentation slides were submitted as an official supplementary material after being accepted as the best academic paper at the Graduate School of Yonsei University.

**Slide:** [PAGE: Prototype-Based Model-Level Explanations for Graph Neural Networks](/assets/pdf/Presentation_PAGE.pdf)

Also, this is the poster that I presented for the short version, accepted at AAAI 2022.

**Poster:** [PAGE (AAAI 2022)](/assets/pdf/Poster_final_AAAI_2022.pdf)

### Edgeless-GNN: Unsupervised Inductive Edgeless Network Embedding

This presentation was given during the 28th Samsung HumanTech award, which eventually led to the award of the bronze prize.

**Slide:** [Edgeless-GNN: Unsupervised Inductive Edgeless Network Embedding](/assets/pdf/EdgelessGNN_Hutech.pdf)


## Internal presentations

### Causal learning

This was a two-part presentation where I introduced the basics of causal learning in an internal seminar at my lab, which I got a LOT of help from ["Introduction to Causal Inference" by Brady Neal](https://www.bradyneal.com/causal-inference-course). Thank you!!

**Slide 1:** [Causal learning: Part 1](/assets/pdf/Causal_learning_part1.pdf)

**Slide 2:** [Causal learning: Part 2](/assets/pdf/Causal_learning_part2.pdf)

### Self-supervised learning

This was a presentation made when I was interested in understanding the SimCLR framework.

**Slide:** [Introduction to SimCLR](/assets/pdf/SimCLR.pdf)

### Knowledge distillation

Towards understanding knowledge distillation: This was a presentation made when I was interested in understanding knowledge distillation, which I eventually applied to my research on efficient graph learning.

**Slide:** [Towards understanding knowledge distillation](/assets/pdf/Knowledge_distillation.pdf)

### Physics-informed machine learning (PIML)

A review of the paper: Vector Neurons: A General Framework for SO(3)-Equivariant Networks by Dent et al. This was a presentation made when I was interested in understanding the physics-informed machine learning landscape.

**Slide:** [Vector Neurons: A General Framework for SO(3)-Equivariant Networks](/assets/pdf/Vector_Neurons.pdf)

### Unsupervised disentanglement

A review of the paper: Challenging common assumptions in the unsupervised learning of disentangled representations by Locatello et al.

**Slide:** [Challenging common assumptions in the unsupervised learning of disentangled representations](/assets/pdf/Unsupervised_disentanglement.pdf)